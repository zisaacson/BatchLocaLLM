version: '3.8'

services:
  # =============================================================================
  # vLLM Batch Server
  # =============================================================================
  server:
    build:
      context: .
      dockerfile: Dockerfile
    image: vllm-batch-server:latest
    container_name: vllm-batch-server
    
    # GPU configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    # Port mappings
    ports:
      - "8000:8000"  # API server
      - "9090:9090"  # Prometheus metrics
    
    # Environment variables (override with .env file)
    environment:
      # Model configuration
      - MODEL_NAME=${MODEL_NAME:-meta-llama/Llama-3.1-8B-Instruct}
      - MODEL_REVISION=${MODEL_REVISION:-main}
      - HF_TOKEN=${HF_TOKEN:-}
      - TRUST_REMOTE_CODE=${TRUST_REMOTE_CODE:-false}
      
      # GPU configuration
      - TENSOR_PARALLEL_SIZE=${TENSOR_PARALLEL_SIZE:-1}
      - GPU_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION:-0.9}
      - MAX_MODEL_LEN=${MAX_MODEL_LEN:-8192}
      - DTYPE=${DTYPE:-auto}
      - QUANTIZATION=${QUANTIZATION:-}
      
      # Batch processing
      - MAX_NUM_SEQS=${MAX_NUM_SEQS:-256}
      - MAX_CONCURRENT_BATCHES=${MAX_CONCURRENT_BATCHES:-4}
      - MAX_BATCH_FILE_SIZE_MB=${MAX_BATCH_FILE_SIZE_MB:-500}
      - MAX_REQUESTS_PER_BATCH=${MAX_REQUESTS_PER_BATCH:-50000}
      - BATCH_COMPLETION_TIMEOUT_HOURS=${BATCH_COMPLETION_TIMEOUT_HOURS:-24}
      
      # Prefix caching
      - ENABLE_PREFIX_CACHING=${ENABLE_PREFIX_CACHING:-true}
      - PREFIX_CACHE_RATIO=${PREFIX_CACHE_RATIO:-0.1}
      
      # Server configuration
      - HOST=${HOST:-0.0.0.0}
      - PORT=${PORT:-8000}
      - API_KEY=${API_KEY:-}
      - ENABLE_CORS=${ENABLE_CORS:-true}
      - CORS_ORIGINS=${CORS_ORIGINS:-*}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - JSON_LOGGING=${JSON_LOGGING:-true}
      
      # Storage
      - STORAGE_PATH=${STORAGE_PATH:-/data/batches}
      - DATABASE_PATH=${DATABASE_PATH:-/data/vllm_batch.db}
      - CLEANUP_AFTER_DAYS=${CLEANUP_AFTER_DAYS:-7}
      
      # Performance tuning
      - ENABLE_CUDA_GRAPH=${ENABLE_CUDA_GRAPH:-true}
      - SWAP_SPACE_GB=${SWAP_SPACE_GB:-4}
      - BLOCK_SIZE=${BLOCK_SIZE:-16}
      - MAX_PARALLEL_LOADING_WORKERS=${MAX_PARALLEL_LOADING_WORKERS:-4}
      
      # Monitoring
      - ENABLE_METRICS=${ENABLE_METRICS:-true}
      - METRICS_PORT=${METRICS_PORT:-9090}
      - ENABLE_HEALTH_CHECKS=${ENABLE_HEALTH_CHECKS:-true}
      
      # Advanced vLLM options
      - DISABLE_LOG_STATS=${DISABLE_LOG_STATS:-false}
      - DISABLE_LOG_REQUESTS=${DISABLE_LOG_REQUESTS:-false}
      - ENABLE_CHUNKED_PREFILL=${ENABLE_CHUNKED_PREFILL:-true}
      - MAX_NUM_BATCHED_TOKENS=${MAX_NUM_BATCHED_TOKENS:-8192}
      - SCHEDULER_DELAY=${SCHEDULER_DELAY:-0.0}
      
      # Development
      - DEBUG=${DEBUG:-false}
      - LOG_REQUESTS=${LOG_REQUESTS:-false}
      - DETAILED_ERRORS=${DETAILED_ERRORS:-true}
    
    # Volume mappings
    volumes:
      # Persistent storage for batch jobs and results
      - batch-data:/data
      
      # Hugging Face cache (speeds up model downloads)
      - hf-cache:/home/vllm/.cache/huggingface
      
      # Optional: Mount local model directory
      # - ./models:/models:ro
    
    # Restart policy
    restart: unless-stopped
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    
    # Network configuration
    networks:
      - vllm-network

  # =============================================================================
  # Label Studio - Annotation Backend
  # =============================================================================
  label-studio:
    image: heartexlabs/label-studio:latest
    container_name: label-studio
    ports:
      - "8080:8080"
    volumes:
      - label-studio-data:/label-studio/data
    environment:
      - LABEL_STUDIO_LOCAL_FILES_SERVING_ENABLED=true
      - LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT=/label-studio/data
    networks:
      - vllm-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # =============================================================================
  # Curation API - Beautiful Frontend + Label Studio Integration
  # =============================================================================
  curation-api:
    build:
      context: .
      dockerfile: Dockerfile.curation
    image: conquest-curation:latest
    container_name: curation-api
    ports:
      - "8001:8001"
    volumes:
      - ./curation_app:/app/curation_app:ro
      - ./static:/app/static:ro
      - ./conquest_schemas:/app/conquest_schemas:ro
    environment:
      - LABEL_STUDIO_URL=http://label-studio:8080
      - LABEL_STUDIO_API_KEY=${LABEL_STUDIO_API_KEY:-}
      - VLLM_API_URL=http://server:8000
    networks:
      - vllm-network
    restart: unless-stopped
    depends_on:
      - label-studio
      - server
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/api/schemas"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

# =============================================================================
# Volumes
# =============================================================================
volumes:
  # Persistent storage for batch jobs, results, and database
  batch-data:
    driver: local

  # Hugging Face model cache
  hf-cache:
    driver: local

  # Label Studio data
  label-studio-data:
    driver: local

# =============================================================================
# Networks
# =============================================================================
networks:
  vllm-network:
    driver: bridge

