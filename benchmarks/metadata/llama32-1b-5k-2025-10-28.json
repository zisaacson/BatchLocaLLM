{
  "test_id": "llama32-1b-5k",
  "timestamp": "2025-10-28T17:47:15.844887Z",
  "platform": "vllm-native",
  "model": "meta-llama/Llama-3.2-1B-Instruct",
  "test": {
    "input_file": "batch_5k.jsonl",
    "output_file": "llama32_1b_5k_results.jsonl",
    "num_requests": 5000
  },
  "results": {
    "success_count": 5000,
    "failure_count": 0,
    "prompt_tokens": 1540000,
    "completion_tokens": 606875,
    "total_tokens": 2146875,
    "model_load_time_seconds": 210.2,
    "inference_time_seconds": 108.35,
    "total_time_seconds": 318.55,
    "throughput_tokens_per_sec": 19813,
    "throughput_requests_per_sec": 46.15
  },
  "configuration": {
    "max_model_len": 4096,
    "gpu_memory_utilization": 0.9,
    "temperature": 0.7,
    "top_p": 0.9,
    "max_tokens": 2000
  }
}