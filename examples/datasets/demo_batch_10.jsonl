{"custom_id": "demo-001", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "google/gemma-3-4b-it", "messages": [{"role": "system", "content": "You are a helpful AI assistant."}, {"role": "user", "content": "Explain what batch processing is in 2-3 sentences."}], "max_tokens": 150}}
{"custom_id": "demo-002", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "google/gemma-3-4b-it", "messages": [{"role": "system", "content": "You are a helpful AI assistant."}, {"role": "user", "content": "What are the main benefits of using vLLM for LLM inference?"}], "max_tokens": 200}}
{"custom_id": "demo-003", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "google/gemma-3-4b-it", "messages": [{"role": "system", "content": "You are a helpful AI assistant."}, {"role": "user", "content": "How does model hot-swapping work in a batch processing system?"}], "max_tokens": 250}}
{"custom_id": "demo-004", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "google/gemma-3-4b-it", "messages": [{"role": "system", "content": "You are a helpful AI assistant."}, {"role": "user", "content": "What is the difference between synchronous and asynchronous API calls?"}], "max_tokens": 200}}
{"custom_id": "demo-005", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "google/gemma-3-4b-it", "messages": [{"role": "system", "content": "You are a helpful AI assistant."}, {"role": "user", "content": "Explain GPU memory management for large language models."}], "max_tokens": 300}}
{"custom_id": "demo-006", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "google/gemma-3-4b-it", "messages": [{"role": "system", "content": "You are a helpful AI assistant."}, {"role": "user", "content": "What are the advantages of using consumer GPUs like RTX 4080 for AI inference?"}], "max_tokens": 250}}
{"custom_id": "demo-007", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "google/gemma-3-4b-it", "messages": [{"role": "system", "content": "You are a helpful AI assistant."}, {"role": "user", "content": "How does incremental saving prevent data loss in long-running batch jobs?"}], "max_tokens": 200}}
{"custom_id": "demo-008", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "google/gemma-3-4b-it", "messages": [{"role": "system", "content": "You are a helpful AI assistant."}, {"role": "user", "content": "What is the OpenAI Batch API format and why is it useful?"}], "max_tokens": 250}}
{"custom_id": "demo-009", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "google/gemma-3-4b-it", "messages": [{"role": "system", "content": "You are a helpful AI assistant."}, {"role": "user", "content": "Explain the concept of queue management in distributed systems."}], "max_tokens": 200}}
{"custom_id": "demo-010", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "google/gemma-3-4b-it", "messages": [{"role": "system", "content": "You are a helpful AI assistant."}, {"role": "user", "content": "What monitoring metrics are important for LLM inference servers?"}], "max_tokens": 250}}

