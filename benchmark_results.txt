================================================================================
PARALLEL PROCESSING TEST
================================================================================

1. Initializing Ollama backend...
✅ Ollama server healthy

2. Loading model: gemma3:12b...
✅ Model loaded

3. Creating 20 test requests...
✅ Created 20 requests

================================================================================
TEST: 1 workers
================================================================================
{"timestamp": "2025-10-27T11:51:58", "level": "INFO", "logger": "vllm_batch_server", "message": "\n============================================================\nPARALLEL BATCH PROCESSING\n============================================================\nTotal requests: 20\nWorkers: 1\nRequests per worker: ~20\n============================================================", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:51:58", "level": "INFO", "logger": "vllm_batch_server", "message": "Starting 1 workers...", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:51:58", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 0 starting: 20 requests", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:51:59", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 0 progress: 10/20 (50.0%) | Rate: 7.41 req/s | Success: 50.0%", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:01", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 0 progress: 20/20 (100.0%) | Rate: 7.21 req/s | Success: 100.0%", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:01", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 0 completed: 20/20 successful | Time: 2.8s | Rate: 7.21 req/s", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:01", "level": "INFO", "logger": "vllm_batch_server", "message": "\n============================================================\nBATCH COMPLETE\n============================================================\nTotal requests: 20\nSuccessful: 20 (100.0%)\nFailed: 0 (0.0%)\nTotal time: 0.0 minutes\nOverall rate: 7.21 req/s\nSpeedup: 36.0x vs sequential\n============================================================", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:01", "level": "INFO", "logger": "vllm_batch_server", "message": "\nPer-Worker Statistics:", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:01", "level": "INFO", "logger": "vllm_batch_server", "message": "  Worker 0: 20/20 | 7.21 req/s | 0 retries", "service": "vllm-batch-server"}

RESULTS:
  Total: 20
  Success: 20
  Failed: 0
  Time: 2.8s
  Rate: 7.21 req/s
  Speedup: 36.0x vs baseline (0.20 req/s)

SAMPLE RESPONSES:
  Request 0: 0
  Request 1: 2
  Request 2: 4

================================================================================
TEST: 2 workers
================================================================================
{"timestamp": "2025-10-27T11:52:01", "level": "INFO", "logger": "vllm_batch_server", "message": "\n============================================================\nPARALLEL BATCH PROCESSING\n============================================================\nTotal requests: 20\nWorkers: 2\nRequests per worker: ~10\n============================================================", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:01", "level": "INFO", "logger": "vllm_batch_server", "message": "Starting 2 workers...", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:01", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 0 starting: 10 requests", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:01", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 1 starting: 10 requests", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:02", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 1 progress: 10/10 (100.0%) | Rate: 6.90 req/s | Success: 100.0%", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:02", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 1 completed: 10/10 successful | Time: 1.4s | Rate: 6.90 req/s", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:02", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 0 progress: 10/10 (100.0%) | Rate: 6.59 req/s | Success: 100.0%", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:02", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 0 completed: 10/10 successful | Time: 1.5s | Rate: 6.59 req/s", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:02", "level": "INFO", "logger": "vllm_batch_server", "message": "\n============================================================\nBATCH COMPLETE\n============================================================\nTotal requests: 20\nSuccessful: 20 (100.0%)\nFailed: 0 (0.0%)\nTotal time: 0.0 minutes\nOverall rate: 13.18 req/s\nSpeedup: 65.9x vs sequential\n============================================================", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:02", "level": "INFO", "logger": "vllm_batch_server", "message": "\nPer-Worker Statistics:", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:02", "level": "INFO", "logger": "vllm_batch_server", "message": "  Worker 0: 10/10 | 6.59 req/s | 0 retries", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:02", "level": "INFO", "logger": "vllm_batch_server", "message": "  Worker 1: 10/10 | 6.90 req/s | 0 retries", "service": "vllm-batch-server"}

RESULTS:
  Total: 20
  Success: 20
  Failed: 0
  Time: 1.5s
  Rate: 13.18 req/s
  Speedup: 65.9x vs baseline (0.20 req/s)

SAMPLE RESPONSES:
  Request 0: 0
  Request 1: 2
  Request 2: 4

================================================================================
TEST: 4 workers
================================================================================
{"timestamp": "2025-10-27T11:52:02", "level": "INFO", "logger": "vllm_batch_server", "message": "\n============================================================\nPARALLEL BATCH PROCESSING\n============================================================\nTotal requests: 20\nWorkers: 4\nRequests per worker: ~5\n============================================================", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:02", "level": "INFO", "logger": "vllm_batch_server", "message": "Starting 4 workers...", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:02", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 0 starting: 5 requests", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:02", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 1 starting: 5 requests", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:02", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 2 starting: 5 requests", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:02", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 3 starting: 5 requests", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:03", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 0 completed: 5/5 successful | Time: 1.2s | Rate: 4.30 req/s", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 3 completed: 5/5 successful | Time: 1.2s | Rate: 4.07 req/s", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 1 completed: 5/5 successful | Time: 1.3s | Rate: 3.85 req/s", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 2 completed: 5/5 successful | Time: 1.4s | Rate: 3.66 req/s", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "\n============================================================\nBATCH COMPLETE\n============================================================\nTotal requests: 20\nSuccessful: 20 (100.0%)\nFailed: 0 (0.0%)\nTotal time: 0.0 minutes\nOverall rate: 14.65 req/s\nSpeedup: 73.3x vs sequential\n============================================================", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "\nPer-Worker Statistics:", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "  Worker 0: 5/5 | 4.30 req/s | 0 retries", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "  Worker 1: 5/5 | 3.85 req/s | 0 retries", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "  Worker 2: 5/5 | 3.66 req/s | 0 retries", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "  Worker 3: 5/5 | 4.07 req/s | 0 retries", "service": "vllm-batch-server"}

RESULTS:
  Total: 20
  Success: 20
  Failed: 0
  Time: 1.4s
  Rate: 14.65 req/s
  Speedup: 73.2x vs baseline (0.20 req/s)

SAMPLE RESPONSES:
  Request 0: 0
  Request 1: 2
  Request 2: 4

================================================================================
TEST: 8 workers
================================================================================
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "\n============================================================\nPARALLEL BATCH PROCESSING\n============================================================\nTotal requests: 20\nWorkers: 8\nRequests per worker: ~2\n============================================================", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "Starting 8 workers...", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 0 starting: 3 requests", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 1 starting: 3 requests", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 2 starting: 3 requests", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 3 starting: 3 requests", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 4 starting: 2 requests", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 5 starting: 2 requests", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 6 starting: 2 requests", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 7 starting: 2 requests", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:04", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 4 completed: 2/2 successful | Time: 0.7s | Rate: 2.96 req/s", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:05", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 7 completed: 2/2 successful | Time: 0.9s | Rate: 2.15 req/s", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:05", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 6 completed: 2/2 successful | Time: 1.0s | Rate: 2.00 req/s", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:05", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 5 completed: 2/2 successful | Time: 1.1s | Rate: 1.79 req/s", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:05", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 2 completed: 3/3 successful | Time: 1.2s | Rate: 2.53 req/s", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:05", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 3 completed: 3/3 successful | Time: 1.3s | Rate: 2.39 req/s", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:05", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 0 completed: 3/3 successful | Time: 1.3s | Rate: 2.29 req/s", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:05", "level": "INFO", "logger": "vllm_batch_server", "message": "Worker 1 completed: 3/3 successful | Time: 1.4s | Rate: 2.18 req/s", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:05", "level": "INFO", "logger": "vllm_batch_server", "message": "\n============================================================\nBATCH COMPLETE\n============================================================\nTotal requests: 20\nSuccessful: 20 (100.0%)\nFailed: 0 (0.0%)\nTotal time: 0.0 minutes\nOverall rate: 14.53 req/s\nSpeedup: 72.7x vs sequential\n============================================================", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:05", "level": "INFO", "logger": "vllm_batch_server", "message": "\nPer-Worker Statistics:", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:05", "level": "INFO", "logger": "vllm_batch_server", "message": "  Worker 0: 3/3 | 2.29 req/s | 0 retries", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:05", "level": "INFO", "logger": "vllm_batch_server", "message": "  Worker 1: 3/3 | 2.18 req/s | 0 retries", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:05", "level": "INFO", "logger": "vllm_batch_server", "message": "  Worker 2: 3/3 | 2.53 req/s | 0 retries", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:05", "level": "INFO", "logger": "vllm_batch_server", "message": "  Worker 3: 3/3 | 2.39 req/s | 0 retries", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:05", "level": "INFO", "logger": "vllm_batch_server", "message": "  Worker 4: 2/2 | 2.96 req/s | 0 retries", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:05", "level": "INFO", "logger": "vllm_batch_server", "message": "  Worker 5: 2/2 | 1.79 req/s | 0 retries", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:05", "level": "INFO", "logger": "vllm_batch_server", "message": "  Worker 6: 2/2 | 2.00 req/s | 0 retries", "service": "vllm-batch-server"}
{"timestamp": "2025-10-27T11:52:05", "level": "INFO", "logger": "vllm_batch_server", "message": "  Worker 7: 2/2 | 2.15 req/s | 0 retries", "service": "vllm-batch-server"}

RESULTS:
  Total: 20
  Success: 20
  Failed: 0
  Time: 1.4s
  Rate: 14.53 req/s
  Speedup: 72.7x vs baseline (0.20 req/s)

SAMPLE RESPONSES:
  Request 0: 0
  Request 1: 2
  Request 2: 4

================================================================================
✅ PARALLEL PROCESSING TEST COMPLETE
================================================================================
