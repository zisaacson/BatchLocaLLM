# ðŸ—ï¸ Architecture Proposal: Core vs Integrations

## ðŸŽ¯ The Problem

**Current state:** Everything is bundled together
- Core batch processing + Label Studio + webhooks + monitoring all mixed
- Can't use the system without installing everything
- Skeptics see bloat, power users see value

**Goal:** Separate concerns so:
- Skeptics get a lean, fast batch processor
- Power users get optional integrations
- Both groups are happy

---

## ðŸ“¦ Proposed Structure

```
vllm-batch-server/
â”‚
â”œâ”€â”€ core/                           # CORE: Skeptic-approved minimal system
â”‚   â”œâ”€â”€ batch_processor.py          # Pure vLLM batch processing
â”‚   â”œâ”€â”€ api_server.py               # REST API (optional but recommended)
â”‚   â”œâ”€â”€ worker.py                   # Job worker
â”‚   â”œâ”€â”€ database.py                 # Job queue (PostgreSQL)
â”‚   â”œâ”€â”€ config.py                   # Configuration
â”‚   â””â”€â”€ models.py                   # Data models
â”‚
â”œâ”€â”€ integrations/                   # OPTIONAL: Power user features
â”‚   â”‚
â”‚   â”œâ”€â”€ monitoring/                 # Grafana + Prometheus + Loki
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ prometheus_exporter.py
â”‚   â”‚   â”œâ”€â”€ grafana_dashboards/
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ curation/                   # Label Studio integration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ label_studio_client.py
â”‚   â”‚   â”œâ”€â”€ curation_api.py
â”‚   â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ webhooks/                   # Webhook notifications
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ webhook_handler.py
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ result_handlers/            # Custom result processing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ s3_handler.py
â”‚   â”‚   â”œâ”€â”€ gcs_handler.py
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ aris/                       # Private: Your specific use case
â”‚   â”‚   â”œâ”€â”€ conquest_schemas/
â”‚   â”‚   â”œâ”€â”€ curation_app/
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â””â”€â”€ examples/                   # Example integrations
â”‚       â”œâ”€â”€ slack_notifications/
â”‚       â”œâ”€â”€ discord_bot/
â”‚       â””â”€â”€ custom_metrics/
â”‚
â”œâ”€â”€ cli/                            # Command-line interface
â”‚   â””â”€â”€ vllm_batch.py
â”‚
â”œâ”€â”€ sdk/                            # Python SDK (future)
â”‚   â””â”€â”€ client.py
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.core.yml     # Core only
â”‚   â”œâ”€â”€ docker-compose.full.yml     # Core + all integrations
â”‚   â””â”€â”€ docker-compose.custom.yml   # Mix and match
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ GETTING_STARTED.md          # Core system
â”‚   â”œâ”€â”€ API.md
â”‚   â”œâ”€â”€ INTEGRATIONS.md             # How to use integrations
â”‚   â””â”€â”€ CUSTOM_INTEGRATIONS.md      # How to build your own
â”‚
â””â”€â”€ examples/
    â”œâ”€â”€ simple_batch.py             # 80-line skeptic version
    â”œâ”€â”€ with_monitoring.py          # Core + monitoring
    â””â”€â”€ full_stack.py               # Everything enabled
```

---

## ðŸŽ¯ Core System (Skeptic-Approved)

### **What's Included**

**Minimal dependencies:**
```txt
vllm==0.11.0
fastapi>=0.115.0
uvicorn>=0.32.0
sqlalchemy>=2.0.0
psycopg2-binary>=2.9.0
pydantic>=2.10.0
```

**Core features:**
- âœ… vLLM batch processing with chunking
- âœ… OpenAI-compatible API
- âœ… PostgreSQL job queue (durable, crash-resistant)
- âœ… Incremental saves (checkpoint every N requests)
- âœ… Model hot-swapping (for consumer GPUs)
- âœ… Sequential processing (prevent OOM)
- âœ… Basic file storage
- âœ… Job status tracking

**What's NOT included:**
- âŒ Monitoring (Grafana/Prometheus)
- âŒ Label Studio
- âŒ Webhooks
- âŒ Sentry
- âŒ Rate limiting
- âŒ Custom result handlers

### **Installation**

```bash
# Core only
pip install vllm-batch-server

# Start server
vllm-batch start

# Submit job
vllm-batch submit input.jsonl --model gemma-3-4b
```

### **What Skeptics Get**

**80-line equivalent, but with:**
- âœ… Crash recovery (PostgreSQL queue)
- âœ… Incremental saves (don't lose work)
- âœ… Model hot-swapping (RTX 4080 friendly)
- âœ… REST API (optional, can use CLI)
- âœ… Job tracking (know what's running)

**No bloat:**
- âŒ No monitoring stack
- âŒ No data curation
- âŒ No webhooks
- âŒ No error tracking

---

## ðŸ”Œ Integration System (Power Users)

### **How It Works**

**Integrations are plugins:**

```python
# core/config.py
class Settings(BaseSettings):
    # Core settings
    DATABASE_URL: str
    VLLM_MODEL: str
    
    # Integration flags (all default to False)
    ENABLE_MONITORING: bool = False
    ENABLE_CURATION: bool = False
    ENABLE_WEBHOOKS: bool = False
```

**Enable via environment:**

```bash
# .env
ENABLE_MONITORING=true
ENABLE_CURATION=true
```

**Or via pip extras:**

```bash
# Install with monitoring
pip install vllm-batch-server[monitoring]

# Install with curation
pip install vllm-batch-server[curation]

# Install everything
pip install vllm-batch-server[all]
```

### **Integration: Monitoring**

**Location:** `integrations/monitoring/`

**Dependencies:**
```txt
prometheus-client>=0.21.0
grafana (Docker)
loki (Docker)
```

**Usage:**
```bash
# Install
pip install vllm-batch-server[monitoring]

# Enable
export ENABLE_MONITORING=true

# Start with monitoring
docker compose -f docker-compose.monitoring.yml up -d
vllm-batch start
```

**What you get:**
- Grafana dashboards (port 4020)
- Prometheus metrics (port 4022)
- Loki logs (port 4021)
- GPU utilization tracking
- Throughput metrics
- Job duration histograms

---

### **Integration: Curation (Label Studio)**

**Location:** `integrations/curation/`

**Dependencies:**
```txt
label-studio (Docker)
```

**Usage:**
```bash
# Install
pip install vllm-batch-server[curation]

# Enable
export ENABLE_CURATION=true

# Start with curation
docker compose -f docker-compose.curation.yml up -d
vllm-batch start --with-curation
```

**What you get:**
- Label Studio UI (port 4015)
- Curation API (port 4082)
- Result review interface
- Training data export
- Quality scoring

---

### **Integration: Webhooks**

**Location:** `integrations/webhooks/`

**Dependencies:**
```txt
httpx>=0.28.0
```

**Usage:**
```bash
# Install (included in core, just enable)
export ENABLE_WEBHOOKS=true
export WEBHOOK_URL=https://your-app.com/batch-complete

# Start
vllm-batch start
```

**What you get:**
- POST to webhook on job completion
- Configurable retry logic
- Payload customization

---

### **Integration: Result Handlers**

**Location:** `integrations/result_handlers/`

**Purpose:** Custom post-processing of results

**Built-in handlers:**
- `S3Handler` - Upload results to S3
- `GCSHandler` - Upload to Google Cloud Storage
- `WebhookHandler` - POST to webhook
- `LabelStudioHandler` - Send to Label Studio

**Usage:**
```python
# integrations/result_handlers/custom.py
from integrations.result_handlers.base import ResultHandler

class MyCustomHandler(ResultHandler):
    def handle(self, batch_id: str, results: list, metadata: dict):
        # Your custom logic
        send_to_slack(f"Batch {batch_id} complete!")
        upload_to_s3(results)
```

**Register:**
```python
# config.py
RESULT_HANDLERS = [
    "integrations.result_handlers.s3_handler.S3Handler",
    "integrations.result_handlers.custom.MyCustomHandler",
]
```

---

## ðŸ“Š Comparison: Core vs Full

| Feature | Core | + Monitoring | + Curation | Full |
|---------|------|--------------|------------|------|
| **vLLM batch processing** | âœ… | âœ… | âœ… | âœ… |
| **OpenAI API** | âœ… | âœ… | âœ… | âœ… |
| **Job queue (PostgreSQL)** | âœ… | âœ… | âœ… | âœ… |
| **Incremental saves** | âœ… | âœ… | âœ… | âœ… |
| **Model hot-swapping** | âœ… | âœ… | âœ… | âœ… |
| **Grafana dashboards** | âŒ | âœ… | âŒ | âœ… |
| **Prometheus metrics** | âŒ | âœ… | âŒ | âœ… |
| **Label Studio** | âŒ | âŒ | âœ… | âœ… |
| **Curation UI** | âŒ | âŒ | âœ… | âœ… |
| **Webhooks** | âŒ | âŒ | âŒ | âœ… |
| **Result handlers** | âŒ | âŒ | âŒ | âœ… |
| | | | | |
| **Install time** | 2 min | 5 min | 8 min | 10 min |
| **Docker containers** | 1 | 4 | 3 | 6 |
| **Dependencies** | 6 | 8 | 8 | 12 |
| **Use case** | Simple batch | Production | Data curation | Full platform |

---

## ðŸŽ¯ Migration Plan

### **Phase 1: Restructure (Now)**

1. âœ… Move monitoring to `integrations/monitoring/`
2. âœ… Move Label Studio to `integrations/curation/`
3. âœ… Move webhooks to `integrations/webhooks/`
4. âœ… Move result handlers to `integrations/result_handlers/`
5. âœ… Keep Aris in `integrations/aris/` (private)

### **Phase 2: Make Optional (v1.1)**

6. âš ï¸ Add feature flags to config
7. âš ï¸ Create separate docker-compose files
8. âš ï¸ Update pyproject.toml with extras
9. âš ï¸ Update documentation

### **Phase 3: CLI Tool (v1.2)**

10. ðŸ’¡ Create `vllm-batch` CLI
11. ðŸ’¡ Add `--enable-monitoring` flag
12. ðŸ’¡ Add `--enable-curation` flag

---

## ðŸ“ Updated README Structure

```markdown
# vLLM Batch Server

Process 50,000+ LLM requests on consumer GPUs with crash recovery and model hot-swapping.

## Quick Start (Core)

```bash
pip install vllm-batch-server
vllm-batch submit input.jsonl --model gemma-3-4b
```

## Optional Integrations

### Monitoring (Grafana + Prometheus)
```bash
pip install vllm-batch-server[monitoring]
```

### Data Curation (Label Studio)
```bash
pip install vllm-batch-server[curation]
```

### Everything
```bash
pip install vllm-batch-server[all]
```

## When to Use What

**Core only:** Simple batch processing, one-off jobs  
**+ Monitoring:** Long-running jobs, production use  
**+ Curation:** Building training datasets, quality review  
**Full:** Complete platform for LLM operations
```

---

## âœ… Benefits

### **For Skeptics**
- âœ… Lean core (6 dependencies)
- âœ… Fast install (2 minutes)
- âœ… No bloat
- âœ… Still get crash recovery + incremental saves
- âœ… Can upgrade later if needed

### **For Power Users**
- âœ… All features available
- âœ… Mix and match integrations
- âœ… Build custom integrations
- âœ… Production-grade monitoring
- âœ… Data curation built-in

### **For You (Aris)**
- âœ… Keep everything you built
- âœ… Clean separation of concerns
- âœ… Easy to maintain
- âœ… Easy to extend
- âœ… Better for open source

---

## ðŸŽ“ Key Insight

**The skeptic was right about ONE thing:**

> "Most people just need simple batch processing"

**But you were ALSO right:**

> "Some people need production features"

**Solution:** Make it modular. Everyone wins.

---

**Next step:** Should I start restructuring the codebase to match this architecture?

