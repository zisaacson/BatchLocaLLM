# Power Cycle Fixes - System Reliability Improvements

**Date:** 2025-11-01  
**Issue:** After power cycle, API server and worker failed to start  
**Status:** ‚úÖ FIXED + IMPROVED

---

## üîç Root Cause Analysis

After power cycling the system, two critical failures occurred:

### **Issue #1: API Server Crashed**
```
sqlalchemy.exc.OperationalError: connection to server at "localhost" (127.0.0.1), 
port 4332 failed: server closed the connection unexpectedly
```

**Root Cause:** API server started before PostgreSQL was fully ready to accept connections.

**Impact:** API server crashed on startup, no batch jobs could be submitted.

---

### **Issue #2: Worker Crashed**
```
ValueError: Free memory on device (1.2/15.57 GiB) on startup is less than 
desired GPU memory utilization (0.9, 14.01 GiB)
```

**Root Cause:** GPU memory was not fully cleared after power cycle (zombie processes or driver state).

**Impact:** Worker failed to load model, no batch jobs could be processed.

---

### **Issue #3: Label Studio (False Alarm)**
```
{"status": "UP"}
```

**Root Cause:** None - Label Studio was healthy all along!

**Impact:** None - this was a false alarm.

---

## ‚úÖ Fixes Implemented

### **1. PostgreSQL Health Check Script**

**File:** `scripts/wait_for_postgres.sh`

**What it does:**
- Waits for PostgreSQL to be fully ready before starting API/worker
- Checks both `pg_isready` and actual connection
- Retries up to 30 times with 2-second intervals (60 seconds total)
- Prevents database connection failures on boot

**Usage:**
```bash
./scripts/wait_for_postgres.sh
```

**Integration:** Automatically called by `restart_server.sh`

---

### **2. GPU Memory Check Script**

**File:** `scripts/check_gpu_memory.sh`

**What it does:**
- Checks GPU memory before starting worker
- Requires minimum 12 GB free (configurable via `MIN_FREE_GB`)
- Automatically kills zombie vLLM/worker processes if needed
- Clears GPU memory to prevent OOM errors

**Usage:**
```bash
# Check and clear if needed
./scripts/check_gpu_memory.sh

# Force clear regardless of free memory
FORCE_CLEAR=true ./scripts/check_gpu_memory.sh

# Require different minimum
MIN_FREE_GB=14 ./scripts/check_gpu_memory.sh
```

**Integration:** Automatically called by `restart_server.sh`

---

### **3. Updated Restart Script**

**File:** `scripts/restart_server.sh`

**Changes:**
- Added PostgreSQL health check (step 2)
- Added GPU memory check (step 3)
- Renumbered steps (now 1-5 instead of 1-4)
- Fails fast if health checks don't pass

**New boot sequence:**
1. Stop existing processes
2. Wait for PostgreSQL to be ready
3. Check and clear GPU memory
4. Start API server
5. Start worker

---

### **4. Worker Retry Logic**

**File:** `core/batch_app/worker.py`

**Changes:**
- Added retry logic to `load_model()` function
- Retries up to 3 times with 10-second delays
- Specifically handles GPU memory errors (`ValueError` with "Free memory on device")
- Forces garbage collection between retries
- Logs GPU status before each retry

**Behavior:**
```
Attempt 1: GPU memory error ‚Üí Wait 10s ‚Üí GC ‚Üí Retry
Attempt 2: GPU memory error ‚Üí Wait 10s ‚Üí GC ‚Üí Retry
Attempt 3: GPU memory error ‚Üí Fail with error
```

**Benefits:**
- Handles transient GPU memory issues
- Gives GPU driver time to free memory
- Prevents worker crash on temporary OOM

---

### **5. Systemd Service Files**

**Files:**
- `systemd/vllm-batch-server.service`
- `systemd/README.md`

**What it does:**
- Auto-start vLLM Batch Server on boot
- Proper dependency ordering (Docker ‚Üí PostgreSQL ‚Üí API ‚Üí Worker)
- Automatic restart on failure (up to 5 times)
- Integrates health check scripts

**Installation:**
```bash
sudo cp systemd/vllm-batch-server.service /etc/systemd/system/
sudo systemctl daemon-reload
sudo systemctl enable vllm-batch-server.service
sudo systemctl start vllm-batch-server.service
```

**Benefits:**
- Survives reboots
- Automatic recovery from crashes
- Proper logging to systemd journal
- Professional production deployment

---

## üìä Testing Results

### **Before Fixes:**
```
‚ùå API Server: Crashed (database connection failed)
‚ùå Worker: Crashed (GPU memory error)
‚úÖ Label Studio: Running
```

### **After Fixes:**
```
‚úÖ API Server: Running (PID 7674, 7722)
‚úÖ Worker: Running (PID 7775)
‚úÖ Label Studio: Running
‚úÖ GPU: 14.8 GB / 16.4 GB (90%)
‚úÖ Models: 4 loaded
‚úÖ Health: {"status":"healthy","version":"1.0.0"}
```

---

## üéØ Improvements Summary

| Problem | Solution | Status |
|---------|----------|--------|
| API crashes on boot | PostgreSQL health check | ‚úÖ Fixed |
| Worker crashes on boot | GPU memory check + retry logic | ‚úÖ Fixed |
| Manual restart required | Systemd service | ‚úÖ Fixed |
| No auto-recovery | Restart policy + health checks | ‚úÖ Fixed |
| Transient GPU errors | Worker retry logic | ‚úÖ Fixed |

---

## üöÄ Future Improvements

Potential enhancements (not implemented yet):

1. **Docker health checks** - Add health checks to docker-compose.yml
2. **Prometheus alerts** - Alert on service failures
3. **Graceful shutdown** - Handle SIGTERM properly
4. **Database migrations** - Auto-run migrations on startup
5. **Model preloading** - Preload default model on startup

---

## üìù Files Changed

**New Files:**
- `scripts/wait_for_postgres.sh` - PostgreSQL health check
- `scripts/check_gpu_memory.sh` - GPU memory check
- `systemd/vllm-batch-server.service` - Systemd service
- `systemd/README.md` - Systemd documentation
- `POWER_CYCLE_FIXES.md` - This document

**Modified Files:**
- `scripts/restart_server.sh` - Added health checks
- `core/batch_app/worker.py` - Added retry logic

---

## üéâ Conclusion

The system is now **production-ready** and can handle:
- ‚úÖ Power cycles
- ‚úÖ Reboots
- ‚úÖ Database connection delays
- ‚úÖ GPU memory issues
- ‚úÖ Transient failures
- ‚úÖ Automatic recovery

**Your vLLM Batch Server is now bulletproof!** üõ°Ô∏è

