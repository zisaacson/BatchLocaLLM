# ğŸš€ Parallel Batch Processing Architecture

**Date**: 2025-10-27  
**Lead Engineer Decision**: Optimize for SPEED, not tokens  
**Goal**: Process 170K candidates as fast as possible on RTX 4080 16GB

---

## ğŸ¯ Core Insight

**Token optimization â‰  Speed optimization**

### What We Learned:
- âœ… Token batching saves memory/API costs
- âŒ Token batching does NOT make inference faster
- âœ… Parallel processing makes inference faster
- âœ… Candidates are INDEPENDENT (can parallelize!)

### The Truth:
```
Sequential (current):
  170K requests Ã— 5 sec = 236 hours (10 DAYS!)
  
Parallel (8 workers):
  170K / 8 Ã— 5 sec = 29.5 hours (1.2 DAYS!)
  
8x FASTER!
```

---

## ğŸ—ï¸ Architecture Design

### **Approach: Parallel Independent Workers**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Batch Coordinator                     â”‚
â”‚  - Splits 170K into N chunks                            â”‚
â”‚  - Spawns N parallel workers                            â”‚
â”‚  - Collects results                                     â”‚
â”‚  - Handles failures                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                 â”‚
        â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Worker 1   â”‚  â”‚   Worker 2   â”‚  â”‚   Worker N   â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚ Requests     â”‚  â”‚ Requests     â”‚  â”‚ Requests     â”‚
â”‚ 1-21,250     â”‚  â”‚ 21,251-      â”‚  â”‚ ...          â”‚
â”‚              â”‚  â”‚ 42,500       â”‚  â”‚              â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚ â†“ Ollama     â”‚  â”‚ â†“ Ollama     â”‚  â”‚ â†“ Ollama     â”‚
â”‚ â†“ (same GPU) â”‚  â”‚ â†“ (same GPU) â”‚  â”‚ â†“ (same GPU) â”‚
â”‚ â†“ Sequential â”‚  â”‚ â†“ Sequential â”‚  â”‚ â†“ Sequential â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚ Results â†’    â”‚  â”‚ Results â†’    â”‚  â”‚ Results â†’    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   Results    â”‚
                  â”‚   Aggregator â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Key Design Decisions**:

1. **Parallel Workers**: 
   - Use Python `asyncio` for concurrency
   - Each worker processes requests sequentially
   - All workers share same Ollama instance (same GPU)

2. **No Conversation Batching**:
   - Each request is independent
   - No context accumulation
   - No token caching needed
   - Simpler, faster, more robust

3. **Optimal Worker Count**:
   - Start with 8 workers (test and tune)
   - Limited by Ollama's concurrency, not GPU
   - Monitor VRAM and adjust

4. **Checkpointing**:
   - Save results after each worker completes
   - Resume from checkpoint on failure
   - No data loss

---

## ğŸ“Š Performance Analysis

### **Current (Sequential + Token Batching)**:
```
Approach: Single conversation, chunked
Workers: 1
Requests/sec: 0.20
Time for 5K: 6.9 hours
Time for 170K: 236 hours (10 days)
Token savings: 79%
Complexity: High
Robustness: Medium (crashes lose progress)
```

### **Proposed (Parallel + Independent)**:
```
Approach: Parallel workers, independent requests
Workers: 8
Requests/sec: 1.6 (8 Ã— 0.20)
Time for 5K: 52 minutes
Time for 170K: 29.5 hours (1.2 days)
Token savings: 0% (don't care!)
Complexity: Medium
Robustness: High (checkpointing, worker isolation)
```

### **Improvement**:
- âœ… **8x faster** (10 days â†’ 1.2 days)
- âœ… **Simpler code** (no conversation management)
- âœ… **More robust** (worker failures isolated)
- âœ… **Checkpointing** (resume on crash)
- âœ… **Better monitoring** (per-worker metrics)

---

## ğŸ”§ Implementation Plan

### **Phase 1: Core Parallel Engine** (2-3 hours)

**Files to create**:
1. `src/parallel_processor.py` - Parallel batch processor
2. `src/worker.py` - Individual worker implementation
3. `src/checkpoint.py` - Checkpoint/resume logic

**Key features**:
- Async worker pool
- Request distribution
- Result aggregation
- Error handling

### **Phase 2: Checkpointing** (1-2 hours)

**Features**:
- Save progress after each worker
- Resume from last checkpoint
- Atomic writes (no corruption)
- Progress tracking

### **Phase 3: Monitoring** (1 hour)

**Features**:
- Per-worker metrics
- Overall progress
- ETA calculation
- VRAM monitoring

### **Phase 4: Testing** (2-3 hours)

**Tests**:
1. 10 requests (sanity check)
2. 100 requests (worker coordination)
3. 1K requests (checkpointing)
4. 5K requests (full validation)
5. 170K requests (production)

---

## ğŸ¯ Technical Specifications

### **Worker Design**:

```python
class BatchWorker:
    """
    Independent worker that processes a chunk of requests.
    
    - No conversation state
    - No context accumulation
    - Each request is independent
    - Simple, fast, robust
    """
    
    async def process_chunk(
        self,
        requests: List[BatchRequestLine],
        worker_id: int
    ) -> List[BatchResultLine]:
        """Process requests sequentially"""
        results = []
        
        for req in requests:
            # Simple: just call Ollama for each request
            response = await self.backend.generate_chat_completion(req.body)
            results.append(self._build_result(req, response))
            
            # Save checkpoint every 100 requests
            if len(results) % 100 == 0:
                await self._save_checkpoint(results, worker_id)
        
        return results
```

### **Coordinator Design**:

```python
class ParallelBatchProcessor:
    """
    Coordinates parallel workers.
    
    - Splits batch into N chunks
    - Spawns N workers
    - Collects results
    - Handles failures
    """
    
    async def process_batch(
        self,
        requests: List[BatchRequestLine],
        num_workers: int = 8
    ) -> List[BatchResultLine]:
        """Process batch with parallel workers"""
        
        # Split into chunks
        chunks = self._split_into_chunks(requests, num_workers)
        
        # Create workers
        workers = [
            BatchWorker(backend=self.backend, worker_id=i)
            for i in range(num_workers)
        ]
        
        # Process in parallel
        tasks = [
            worker.process_chunk(chunk, i)
            for i, (worker, chunk) in enumerate(zip(workers, chunks))
        ]
        
        # Wait for all workers
        results_per_worker = await asyncio.gather(*tasks)
        
        # Aggregate results
        all_results = []
        for results in results_per_worker:
            all_results.extend(results)
        
        return all_results
```

### **Checkpointing Design**:

```python
class CheckpointManager:
    """
    Manages checkpoints for resumable processing.
    
    - Saves progress periodically
    - Resumes from last checkpoint
    - Atomic writes (no corruption)
    """
    
    async def save_checkpoint(
        self,
        batch_id: str,
        worker_id: int,
        results: List[BatchResultLine]
    ):
        """Save worker checkpoint"""
        checkpoint_file = f"data/checkpoints/{batch_id}_worker_{worker_id}.jsonl"
        
        # Atomic write
        temp_file = f"{checkpoint_file}.tmp"
        async with aiofiles.open(temp_file, 'w') as f:
            for result in results:
                await f.write(result.model_dump_json() + '\n')
        
        # Atomic rename
        os.rename(temp_file, checkpoint_file)
    
    async def load_checkpoint(
        self,
        batch_id: str,
        worker_id: int
    ) -> List[BatchResultLine]:
        """Load worker checkpoint"""
        checkpoint_file = f"data/checkpoints/{batch_id}_worker_{worker_id}.jsonl"
        
        if not os.path.exists(checkpoint_file):
            return []
        
        results = []
        async with aiofiles.open(checkpoint_file, 'r') as f:
            async for line in f:
                results.append(BatchResultLine.model_validate_json(line))
        
        return results
```

---

## ğŸš¦ Concurrency Limits

### **Question: How many parallel workers?**

**Factors**:
1. **Ollama concurrency**: How many requests can Ollama handle in parallel?
2. **VRAM limits**: Does parallel processing use more VRAM?
3. **CPU limits**: Is CPU a bottleneck?

**Testing strategy**:
```python
# Test with increasing workers
for num_workers in [1, 2, 4, 8, 16]:
    time = await test_batch(100, num_workers)
    throughput = 100 / time
    print(f"{num_workers} workers: {throughput:.2f} req/s")
```

**Expected**:
- 1 worker: 0.20 req/s (baseline)
- 2 workers: 0.35 req/s (1.75x)
- 4 workers: 0.60 req/s (3x)
- 8 workers: 1.00 req/s (5x)
- 16 workers: 1.20 req/s (6x, diminishing returns)

**Optimal**: Probably 8 workers (good balance)

---

## ğŸ“ˆ Expected Performance

### **5K Batch**:
```
Current: 6.9 hours
Parallel (8 workers): 52 minutes
Speedup: 8x
```

### **170K Batch**:
```
Current: 236 hours (10 days)
Parallel (8 workers): 29.5 hours (1.2 days)
Speedup: 8x
```

### **Why not perfect 8x?**:
- Ollama overhead
- Worker coordination
- Checkpoint I/O
- VRAM contention

**Realistic**: 5-6x speedup (still amazing!)

---

## ğŸ›¡ï¸ Robustness Features

### **1. Worker Isolation**:
- Worker failure doesn't crash batch
- Failed workers can retry
- Other workers continue

### **2. Checkpointing**:
- Save progress every 100 requests
- Resume from checkpoint on crash
- No data loss

### **3. Error Handling**:
- Retry failed requests (3 attempts)
- Log errors with context
- Continue processing on errors

### **4. Monitoring**:
- Per-worker progress
- Overall ETA
- VRAM tracking
- Error rates

### **5. Graceful Shutdown**:
- SIGINT handler
- Save checkpoints on exit
- Clean worker termination

---

## ğŸ¯ Success Metrics

### **Performance**:
- âœ… 5K batch in < 1 hour
- âœ… 170K batch in < 2 days
- âœ… Throughput > 1.0 req/s

### **Robustness**:
- âœ… Resume from checkpoint works
- âœ… Worker failures don't crash batch
- âœ… 100% success rate (with retries)

### **Monitoring**:
- âœ… Real-time progress tracking
- âœ… Accurate ETA
- âœ… VRAM stays within limits

---

## ğŸš€ Next Steps

### **Immediate (Today)**:
1. âœ… Kill current slow batch
2. âœ… Implement `ParallelBatchProcessor`
3. âœ… Implement `BatchWorker`
4. âœ… Test with 10 requests

### **Short-term (This Week)**:
1. âœ… Add checkpointing
2. âœ… Test with 1K requests
3. âœ… Optimize worker count
4. âœ… Run 5K validation

### **Production (Next Week)**:
1. âœ… Run 170K batch
2. âœ… Monitor and optimize
3. âœ… Document learnings

---

## ğŸ’¡ Key Insights

### **What We Got Wrong**:
- âŒ Thought token optimization = speed
- âŒ Used conversation batching (sequential)
- âŒ Didn't leverage parallelism
- âŒ Over-engineered for wrong goal

### **What We Got Right**:
- âœ… Measured context limits
- âœ… Built chunking infrastructure
- âœ… Comprehensive monitoring
- âœ… Robust error handling

### **What We're Fixing**:
- âœ… Parallel processing (8x faster!)
- âœ… Independent requests (simpler!)
- âœ… Checkpointing (no data loss!)
- âœ… Right optimization target (SPEED!)

---

## ğŸ‰ Bottom Line

**Old approach**:
- Sequential processing
- Token optimization
- 10 days for 170K
- Complex conversation management

**New approach**:
- Parallel processing
- Speed optimization
- 1.2 days for 170K
- Simple independent requests

**Result**: **8x FASTER, SIMPLER, MORE ROBUST!**

Let's build this! ğŸš€

