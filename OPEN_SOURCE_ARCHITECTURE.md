# ğŸ—ï¸ Open Source Architecture Plan

## ğŸ¯ Goal
Create a clean, extensible open source version that:
1. âœ… Showcases our innovation (chunking, monitoring, production-ready)
2. âœ… Provides value to general users (not just Aris-specific)
3. âœ… Impresses Parasail team (complementary, not competitive)
4. âœ… Demonstrates AI-assisted development quality

---

## ğŸ“Š New Repo vs Feature Branch?

**Decision: NEW REPO** âœ…

**Reasons:**
1. Clean slate (no internal commit history)
2. Different audience (general public vs internal)
3. Separate branding (open source vs internal)
4. Easier maintenance (cherry-pick features)
5. Better optics (polished, professional)

**Repo Name:** `vllm-batch-server`  
**GitHub:** `parasail-ai/vllm-batch-server` (if they want to host it) OR `your-org/vllm-batch-server`

---

## ğŸ”§ Core Architecture

### **What vLLM Actually Requires (Correcting Misconception)**

**NOT TRUE:** "vLLM requires 24GB VRAM"

**ACTUALLY TRUE:**
- vLLM VRAM = Model size + KV cache + overhead
- Llama 3.1 8B (FP16) = ~16GB
- Gemma 3 4B (FP16) = ~8GB
- **Problem**: Large batches cause OOM even with small models

**OUR INNOVATION:**
- âœ… Intelligent chunking (5K requests per chunk)
- âœ… Dynamic memory management
- âœ… Incremental saves (resume from crashes)
- âœ… GPU health monitoring

**VALUE PROP (CORRECTED):**
> "Process large batches (50K+ requests) on consumer GPUs without OOM crashes"

---

## ğŸ§© Component Breakdown

### **TIER 1: Core (Always Included)** âœ…

```
batch_app/
â”œâ”€â”€ api_server.py          # OpenAI-compatible Batch API
â”œâ”€â”€ worker.py              # vLLM worker with chunking
â”œâ”€â”€ database.py            # PostgreSQL job queue
â”œâ”€â”€ webhooks.py            # Webhook callbacks
â””â”€â”€ gpu_monitor.py         # GPU health checks
```

**Features:**
- OpenAI Batch API compatibility
- Intelligent chunking (5K per chunk)
- Incremental saves
- Webhook notifications
- GPU health monitoring
- PostgreSQL job queue

**This alone is valuable!**

---

### **TIER 2: Monitoring (Optional via Docker Profile)** âœ…

```
monitoring/
â”œâ”€â”€ prometheus.yml         # Metrics collection
â”œâ”€â”€ grafana/              # Dashboards
â””â”€â”€ docker-compose.monitoring.yml
```

**Usage:**
```bash
# Without monitoring
docker-compose up

# With monitoring
docker-compose --profile monitoring up
```

**Why optional:**
- Some users just want batch processing
- Monitoring adds complexity
- Easy to enable when needed

---

### **TIER 3: Result Handlers (Plugin System)** âœ…

**This is the key innovation for abstracting Aris integration!**

```
result_handlers/
â”œâ”€â”€ base.py                # Abstract base class
â”œâ”€â”€ webhook.py             # Webhook handler (always enabled)
â”œâ”€â”€ label_studio.py        # Label Studio auto-import (optional)
â”œâ”€â”€ postgres.py            # PostgreSQL auto-insert (optional)
â”œâ”€â”€ s3.py                  # S3 auto-upload (optional)
â””â”€â”€ custom.py              # User-defined handlers
```

**Base Handler Interface:**

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any

class ResultHandler(ABC):
    """
    Base class for result handlers.
    
    Result handlers are called when a batch job completes,
    allowing you to automatically process results.
    
    Examples:
    - Import to Label Studio for curation
    - Insert into your database
    - Upload to S3
    - Send to your ML pipeline
    """
    
    @abstractmethod
    def handle(
        self,
        batch_id: str,
        results: List[Dict[str, Any]],
        metadata: Dict[str, Any]
    ) -> bool:
        """
        Process batch results.
        
        Args:
            batch_id: Unique batch identifier
            results: List of result objects (JSONL format)
            metadata: User-provided metadata from batch submission
            
        Returns:
            True if handled successfully, False otherwise
        """
        pass
    
    @abstractmethod
    def enabled(self) -> bool:
        """Check if this handler is enabled."""
        pass
```

**Example: Label Studio Handler**

```python
class LabelStudioHandler(ResultHandler):
    """
    Auto-import batch results to Label Studio for curation.
    
    Configuration (optional):
    - LABEL_STUDIO_URL: Label Studio server URL
    - LABEL_STUDIO_API_KEY: API key
    - LABEL_STUDIO_PROJECT_ID: Project ID
    """
    
    def enabled(self) -> bool:
        return bool(
            os.getenv('LABEL_STUDIO_URL') and
            os.getenv('LABEL_STUDIO_API_KEY')
        )
    
    def handle(self, batch_id, results, metadata):
        if not self.enabled():
            return True  # Skip if not configured
        
        # Import to Label Studio
        # ... (implementation)
        
        return True
```

**Example: Custom Handler (User-Defined)**

```python
class MyCustomHandler(ResultHandler):
    """
    Example: Insert results into your application database.
    """
    
    def enabled(self) -> bool:
        return True  # Always enabled
    
    def handle(self, batch_id, results, metadata):
        # Your custom logic here
        # Example: Insert into your database
        for result in results:
            db.insert('ml_results', {
                'batch_id': batch_id,
                'custom_id': result['custom_id'],
                'response': result['response']['body'],
                'created_at': datetime.now()
            })
        
        return True
```

**Configuration:**

```yaml
# config.yml
result_handlers:
  enabled:
    - webhook          # Always enabled (built-in)
    - label_studio     # Optional (if configured)
    - custom           # User-defined
  
  label_studio:
    url: ${LABEL_STUDIO_URL}
    api_key: ${LABEL_STUDIO_API_KEY}
    project_id: ${LABEL_STUDIO_PROJECT_ID}
```

**Why This Works:**
- âœ… Abstracts Aris integration (Label Studio is just one handler)
- âœ… Users can write custom handlers for their pipelines
- âœ… Optional (doesn't complicate simple use cases)
- âœ… Extensible (plugin architecture)
- âœ… Shows off your innovation (data pipeline integration)

---

## ğŸ¨ What to Strip Out

### **Remove (Too Specific to Aris):**

```
âŒ conquest_schemas/candidate_evaluation.json
âŒ conquest_schemas/cartographer.json
âŒ conquest_schemas/email_evaluation.json
   â†’ These are YOUR business logic

âŒ curation_app/conquest-specific UI
   â†’ Too specific to your use case

âŒ Aris integration docs
   â†’ Internal documentation

âŒ Internal benchmarks/results
   â†’ Your internal data
```

### **Generalize (Make Reusable):**

```
âœ… conquest_schemas/ â†’ schemas/
   - Keep as "example schemas"
   - Show how to define custom schemas
   - Provide template for users

âœ… curation_app/ â†’ result_handlers/label_studio.py
   - Make it a plugin
   - Optional, not required
   - Generic, not conquest-specific

âœ… Auto-import logic â†’ ResultHandler plugin system
   - Extensible architecture
   - Users can write custom handlers
```

---

## ğŸ“¦ Final Structure

```
vllm-batch-server/  (NEW REPO)
â”œâ”€â”€ batch_app/                    # Core batch processing
â”‚   â”œâ”€â”€ api_server.py             # OpenAI-compatible API
â”‚   â”œâ”€â”€ worker.py                 # vLLM worker with chunking
â”‚   â”œâ”€â”€ database.py               # PostgreSQL models
â”‚   â”œâ”€â”€ webhooks.py               # Webhook notifications
â”‚   â””â”€â”€ gpu_monitor.py            # GPU health checks
â”‚
â”œâ”€â”€ result_handlers/              # Plugin system (NEW!)
â”‚   â”œâ”€â”€ base.py                   # Abstract base class
â”‚   â”œâ”€â”€ webhook.py                # Webhook handler (built-in)
â”‚   â”œâ”€â”€ label_studio.py           # Label Studio (optional)
â”‚   â””â”€â”€ examples/
â”‚       â”œâ”€â”€ postgres_insert.py    # Example: DB insert
â”‚       â”œâ”€â”€ s3_upload.py          # Example: S3 upload
â”‚       â””â”€â”€ custom_handler.py     # Template for users
â”‚
â”œâ”€â”€ schemas/                      # Schema system (generalized)
â”‚   â”œâ”€â”€ base_schema.json          # Schema template
â”‚   â””â”€â”€ examples/                 # Example schemas
â”‚       â”œâ”€â”€ chat_completion.json  # Generic chat
â”‚       â”œâ”€â”€ classification.json   # Generic classification
â”‚       â””â”€â”€ candidate_eval.json   # Your example (as demo)
â”‚
â”œâ”€â”€ monitoring/                   # Optional monitoring
â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â””â”€â”€ grafana/
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yml        # Core services
â”‚   â”œâ”€â”€ docker-compose.monitoring.yml  # Optional monitoring
â”‚   â””â”€â”€ docker-compose.curation.yml    # Optional Label Studio
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ e2e/
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ QUICKSTART.md             # 5-minute setup
â”‚   â”œâ”€â”€ ARCHITECTURE.md           # System design
â”‚   â”œâ”€â”€ API.md                    # API reference
â”‚   â”œâ”€â”€ RESULT_HANDLERS.md        # Plugin system guide
â”‚   â”œâ”€â”€ SCHEMAS.md                # Schema system guide
â”‚   â””â”€â”€ DEPLOYMENT.md             # Production deployment
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ basic_batch.py            # Simple batch processing
â”‚   â”œâ”€â”€ with_webhooks.py          # Webhook callbacks
â”‚   â”œâ”€â”€ custom_handler.py         # Custom result handler
â”‚   â””â”€â”€ label_studio_integration.py  # Label Studio example
â”‚
â”œâ”€â”€ README.md                     # Main documentation
â”œâ”€â”€ CONTRIBUTING.md               # Contribution guidelines
â”œâ”€â”€ LICENSE                       # Apache 2.0
â””â”€â”€ pyproject.toml                # Python packaging
```

---

## ğŸ¯ Key Messages for Parasail

### **1. We Built the Server for Your Client**

```markdown
Parasail's openai-batch is an excellent CLIENT library.
We built the SERVER that it can connect to.

Your library â†’ Our server â†’ Local GPU processing
```

### **2. Solves Real Problems**

```markdown
Three innovations:

1. **Large Batch Processing on Consumer GPUs**
   - Process 50K+ requests without OOM
   - Intelligent chunking (5K per chunk)
   - Incremental saves (resume from crashes)

2. **Production Infrastructure**
   - GPU health monitoring
   - Webhook notifications
   - Prometheus + Grafana
   - PostgreSQL job queue

3. **Extensible Result Handling** (NEW!)
   - Plugin system for custom pipelines
   - Label Studio integration (example)
   - Users can write custom handlers
```

### **3. Expands Your Ecosystem**

```markdown
Now your users can choose:

â˜ï¸  Cloud: OpenAI/Parasail (your library â†’ cloud API)
ğŸ  Self-hosted: vLLM Batch Server (your library â†’ local server)

Same client library, different backends!
```

---

## âœ… Recommendation

**YES, include result handler plugin system!**

**Why:**
1. âœ… Abstracts Aris integration (Label Studio is just one plugin)
2. âœ… Provides value to general users (custom pipelines)
3. âœ… Shows off innovation (extensible architecture)
4. âœ… Doesn't complicate simple use cases (optional)
5. âœ… Demonstrates AI-assisted development quality

**Make it optional:**
```bash
# Basic batch processing (no handlers)
docker-compose up

# With Label Studio integration
docker-compose --profile curation up

# With custom handlers
# Users write their own plugins
```

---

## ğŸš€ Next Steps

1. **Create new repo** - `vllm-batch-server`
2. **Copy core components** - batch_app/, monitoring/, tests/
3. **Build plugin system** - result_handlers/ (NEW!)
4. **Generalize schemas** - schemas/examples/
5. **Write documentation** - README, QUICKSTART, ARCHITECTURE
6. **Polish examples** - Show different use cases
7. **Test thoroughly** - Make sure it works out of the box
8. **Reach out to Parasail** - Show them what we built!

---

**This is your coming-out party. Let's make it impressive!** ğŸ‰

