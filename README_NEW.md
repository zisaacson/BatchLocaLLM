# vLLM Batch Processing Server

**Production-ready batch inference system for large-scale LLM processing on consumer GPUs.**

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![vLLM](https://img.shields.io/badge/vLLM-0.11.0-green.svg)](https://github.com/vllm-project/vllm)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## ğŸš€ Features

- âœ… **Native vLLM Batch Processing** - Uses vLLM's built-in batching (no custom wrappers)
- âœ… **Multi-Model Support** - Gemma, Llama, Qwen, OLMo models
- âœ… **Web API** - FastAPI server for job submission and management
- âœ… **Automatic Benchmarking** - Tracks performance and improves estimates
- âœ… **Progress Tracking** - Real-time job status and progress
- âœ… **Optimized for RTX 4080 16GB** - Tested and tuned for consumer GPUs
- âœ… **Scalable** - Process 5K to 200K requests per batch

---

## ğŸ“Š Benchmark Results

| Model | Batch Size | Time | Throughput | Success Rate |
|-------|------------|------|------------|--------------|
| **Gemma 3 4B** | 5,000 | 36.8 min | 2,511 tok/s | 100% âœ… |
| **Gemma 3 4B** | 50,000 | ~6.1 hrs | ~2,511 tok/s | â³ Ready to test |
| **Gemma 3 4B** | 200,000 | ~24.5 hrs | ~2,511 tok/s | â³ Ready to test |

**Hardware:** RTX 4080 16GB  
**Memory Usage:** ~11 GB (5 GB headroom)

---

## ğŸ¯ Quick Start

### **1. Installation**

```bash
# Clone repository
git clone https://github.com/yourusername/vllm-batch-server.git
cd vllm-batch-server

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install vllm fastapi uvicorn sqlalchemy
```

### **2. Start API Server** (Terminal 1)

```bash
./start_api_server.sh
```

### **3. Start Worker** (Terminal 2)

```bash
./start_worker.sh
```

### **4. Submit Batch Job**

```bash
curl -X POST http://localhost:8080/v1/batches \
  -F "file=@your_batch.jsonl" \
  -F "model=google/gemma-3-4b-it"
```

### **5. Check Status**

```bash
curl http://localhost:8080/v1/batches/{batch_id}
```

### **6. Download Results**

```bash
curl http://localhost:8080/v1/batches/{batch_id}/results -o results.jsonl
```

---

## ğŸ“‹ Supported Models

| Model | Size | Memory | Status | RTX 4080 |
|-------|------|--------|--------|----------|
| **Gemma 3 4B** | 8.6 GB | 11 GB | âœ… Production | âœ… Works |
| **Llama 3.2 1B** | 2.5 GB | 5 GB | ğŸŸ¡ To Test | âœ… Should work |
| **Llama 3.2 3B** | 6 GB | 9 GB | ğŸŸ¡ To Test | âœ… Should work |
| **Qwen 3 4B** | 8 GB | 11 GB | ğŸŸ¡ To Test | âœ… Should work |
| **Gemma 3 12B** | 24 GB | 27 GB | ğŸ”´ OOM | âŒ Too large |

See [BENCHMARKING_JOURNEY.md](BENCHMARKING_JOURNEY.md) for detailed model information.

---

## ğŸ—ï¸ Architecture

```
User â†’ FastAPI Server (Port 8080) â†’ SQLite Database
                                          â†“
                                    Worker Process
                                          â†“
                                    vLLM Offline
                                          â†“
                                    GPU (RTX 4080)
                                          â†“
                                  Benchmark System
```

### **Components:**

1. **API Server** (`batch_api/server.py`) - FastAPI web server
2. **Worker** (`batch_api/worker.py`) - Background processor using vLLM
3. **Database** (`batch_api/database.py`) - SQLite job queue
4. **Benchmarks** (`batch_api/benchmarks.py`) - Performance tracking
5. **Models** (`models/`) - Model configurations and registry

---

## ğŸ“ Project Structure

```
vllm-batch-server/
â”œâ”€â”€ batch_api/              # Core application
â”‚   â”œâ”€â”€ server.py           # FastAPI server
â”‚   â”œâ”€â”€ worker.py           # Background worker
â”‚   â”œâ”€â”€ database.py         # Database models
â”‚   â””â”€â”€ benchmarks.py       # Benchmark integration
â”œâ”€â”€ models/                 # Model configurations
â”‚   â”œâ”€â”€ registry.py         # Model registry
â”‚   â”œâ”€â”€ gemma3_4b.py       # Gemma 3 4B config
â”‚   â”œâ”€â”€ llama32_1b.py      # Llama 3.2 1B config
â”‚   â””â”€â”€ qwen3_4b.py        # Qwen 3 4B config
â”œâ”€â”€ benchmarks/             # Benchmark data
â”‚   â”œâ”€â”€ data/               # Results
â”‚   â””â”€â”€ reports/            # Analysis
â”œâ”€â”€ scripts/                # Utility scripts
â”‚   â”œâ”€â”€ start_server.sh
â”‚   â””â”€â”€ start_worker.sh
â”œâ”€â”€ tests/                  # Test suite
â””â”€â”€ docs/                   # Documentation
```

---

## ğŸ”§ API Reference

### **Submit Batch Job**
```http
POST /v1/batches
Content-Type: multipart/form-data

{
  "file": <batch.jsonl>,
  "model": "google/gemma-3-4b-it"
}
```

### **Check Job Status**
```http
GET /v1/batches/{batch_id}
```

### **Download Results**
```http
GET /v1/batches/{batch_id}/results
```

### **List Available Models**
```http
GET /v1/models
```

See [BATCH_API_USAGE.md](BATCH_API_USAGE.md) for complete API documentation.

---

## ğŸ“Š Performance Estimates

Based on RTX 4080 16GB with Gemma 3 4B:

| Batch Size | Estimated Time |
|------------|----------------|
| 100 | ~1 minute |
| 5,000 | ~37 minutes |
| 50,000 | ~6.1 hours |
| 200,000 | ~24.5 hours |

**Estimates automatically improve as you run more jobs!**

---

## ğŸ§ª Testing

```bash
# Run test suite
pytest tests/

# Run specific test
pytest tests/test_api.py

# Run with coverage
pytest --cov=batch_api tests/
```

---

## ğŸ“– Documentation

- **[API Usage Guide](BATCH_API_USAGE.md)** - Complete API documentation
- **[Benchmarking Journey](BENCHMARKING_JOURNEY.md)** - Model benchmarks and testing roadmap
- **[Architecture](BATCH_WEB_APP_ARCHITECTURE.md)** - System architecture details
- **[Codebase Reorganization](CODEBASE_REORGANIZATION.md)** - Project structure guide

---

## ğŸš€ Deployment

### **Local Development**
```bash
./start_api_server.sh  # Terminal 1
./start_worker.sh      # Terminal 2
```

### **Production**
```bash
# Use systemd or supervisor to manage processes
# See docs/DEPLOYMENT.md for details
```

---

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

---

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **[vLLM](https://github.com/vllm-project/vllm)** - Fast and efficient LLM inference
- **[FastAPI](https://fastapi.tiangolo.com/)** - Modern web framework
- **Google, Meta, Qwen, Allen AI** - For open-source models

---

## ğŸ“§ Contact

Questions? Issues? Open an issue on GitHub or contact the maintainers.

---

## ğŸ¯ Why This Project?

**Problem:** OpenAI's Batch API is expensive and slow for large-scale inference.

**Solution:** Self-hosted batch processing on consumer GPUs using vLLM's native batching.

**Results:**
- âœ… 100% success rate on 5K batches
- âœ… 2,511 tokens/sec throughput
- âœ… Cost: $0 (vs $X per 1M tokens on OpenAI)
- âœ… Privacy: Your data stays on your hardware

---

**Ready to process 200K requests on your RTX 4080!** ğŸš€

