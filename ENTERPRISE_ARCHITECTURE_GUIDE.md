# Enterprise Architecture Guide - Training Data Collection System

**Date:** 2025-10-29  
**Purpose:** Scalable, durable architecture for collecting high-quality training data from candidate evaluations

---

## ğŸ¯ **Your Use Case: Training Data Collection**

You have:
- **Main Web App (Aris)** - Evaluates 170K+ candidates for recruiting
- **vLLM Batch Server** - Processes evaluations with local LLMs
- **Goal:** Create high-quality training data from these evaluations

**Key Questions:**
1. Should we store results in the batch server?
2. Should training data be created in the web app or batch server?
3. What's the enterprise best practice?
4. How do agents know how to interact with the system?

---

## ğŸ“Š **Enterprise Architecture: Separation of Concerns**

### **Best Practice: Microservices Pattern**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ARIS Web App                             â”‚
â”‚              (Business Logic Layer)                         â”‚
â”‚                                                             â”‚
â”‚  - Candidate management                                     â”‚
â”‚  - User interface                                           â”‚
â”‚  - Training data curation                                   â”‚
â”‚  - Quality control & labeling                               â”‚
â”‚  - Data export for fine-tuning                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ HTTP POST (submit batch)
                     â”‚ HTTP GET (poll status)
                     â”‚ HTTP GET (download results)
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              vLLM Batch Server (Port 4080)                  â”‚
â”‚              (Inference Service Layer)                      â”‚
â”‚                                                             â”‚
â”‚  - Accept batch inference requests                          â”‚
â”‚  - Queue management                                         â”‚
â”‚  - GPU resource management                                  â”‚
â”‚  - Run LLM inference                                        â”‚
â”‚  - Return raw results                                       â”‚
â”‚  - Store results temporarily (7-30 days)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  RTX 4080 16GB GPU                          â”‚
â”‚              (Compute Resource)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… **Answers to Your Questions**

### **1. Should we store results in the batch server?**

**YES - Temporarily (7-30 days)**

**Reasons:**
- âœ… **Reliability:** Results available if web app fails to download
- âœ… **Retry Logic:** Web app can re-download if needed
- âœ… **Debugging:** Audit trail for troubleshooting
- âœ… **Async Processing:** Web app doesn't need to stay connected

**Storage Strategy:**
```
batch_server/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ batches/
â”‚   â”‚   â”œâ”€â”€ input/          # Original JSONL requests (7 days)
â”‚   â”‚   â”œâ”€â”€ output/         # Results JSONL (30 days)
â”‚   â”‚   â””â”€â”€ failed/         # Failed requests (30 days)
â”‚   â””â”€â”€ database/
â”‚       â””â”€â”€ batches.db      # Job metadata (permanent)
```

**Retention Policy:**
- Input files: 7 days (can be re-submitted if needed)
- Output files: 30 days (web app should download within this window)
- Failed requests: 30 days (for debugging)
- Job metadata: Permanent (for analytics)

---

### **2. Should training data be created in web app or batch server?**

**WEB APP - Definitely!**

**Reasons:**

#### **Batch Server Should:**
- âœ… Run inference (what it's good at)
- âœ… Return raw LLM outputs
- âœ… Be stateless and reusable
- âœ… Not know about your business logic

#### **Web App Should:**
- âœ… **Curate training data** (business logic)
- âœ… **Label quality** (human-in-the-loop)
- âœ… **Filter bad examples** (quality control)
- âœ… **Add metadata** (candidate context, recruiter feedback)
- âœ… **Export for fine-tuning** (format conversion)
- âœ… **Store long-term** (permanent database)

**Why?**

1. **Separation of Concerns**
   - Batch server = inference engine (reusable)
   - Web app = business logic (specific to recruiting)

2. **Quality Control**
   - You need human review before training data is finalized
   - Web app has the UI for this
   - Batch server is headless

3. **Flexibility**
   - You might want to combine multiple LLM outputs
   - You might want to add recruiter feedback
   - You might want to filter by candidate quality
   - All of this is business logic â†’ belongs in web app

4. **Data Governance**
   - Training data is a strategic asset
   - Should be in your main database, not inference server
   - Easier to backup, version, and audit

---

### **3. What's the enterprise best practice?**

**Pattern: Inference Service + Data Lake**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ARIS Web App                             â”‚
â”‚                                                             â”‚
â”‚  1. Submit batch to inference service                       â”‚
â”‚  2. Poll for completion                                     â”‚
â”‚  3. Download raw results                                    â”‚
â”‚  4. Curate training data (filter, label, enrich)            â”‚
â”‚  5. Store in permanent database                             â”‚
â”‚  6. Export for fine-tuning                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚                          â”‚
                     â”‚                          â”‚
                     â†“                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  vLLM Batch Server           â”‚  â”‚  Training Data Lake      â”‚
â”‚  (Temporary Storage)         â”‚  â”‚  (Permanent Storage)     â”‚
â”‚                              â”‚  â”‚                          â”‚
â”‚  - Raw inference results     â”‚  â”‚  - Curated examples      â”‚
â”‚  - 30-day retention          â”‚  â”‚  - Quality labels        â”‚
â”‚  - No business logic         â”‚  â”‚  - Metadata enriched     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  - Version controlled    â”‚
                                  â”‚  - Backed up             â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Enterprise Principles:**

1. **Single Responsibility**
   - Each service does ONE thing well
   - Batch server = inference
   - Web app = business logic

2. **Loose Coupling**
   - Services communicate via HTTP API
   - Can swap batch server implementation
   - Can add more inference services

3. **Data Ownership**
   - Web app owns training data
   - Batch server owns inference results (temporarily)
   - Clear boundaries

4. **Scalability**
   - Can run multiple batch servers
   - Can run batch server on different hardware
   - Can add GPU nodes without changing web app

5. **Observability**
   - Each service has its own monitoring
   - Clear API contracts
   - Easy to debug

---

## ğŸ”§ **Recommended Implementation**

### **Batch Server Responsibilities**

```python
# batch_app/api_server.py

@app.post("/v1/batches")
async def create_batch(file: UploadFile, model: str):
    """
    Accept batch job, run inference, return results.
    NO business logic about training data.
    """
    # 1. Validate request
    # 2. Check GPU health
    # 3. Queue job
    # 4. Return batch_id
    
@app.get("/v1/batches/{batch_id}/results")
async def get_results(batch_id: str):
    """
    Return raw LLM outputs.
    Web app decides what to do with them.
    """
    # Return JSONL with raw inference results
```

### **Web App Responsibilities**

```python
# aris_app/training_data_service.py

class TrainingDataService:
    """
    Curate high-quality training data from LLM evaluations.
    """
    
    async def create_training_data_from_batch(self, batch_id: str):
        """
        1. Download results from batch server
        2. Parse LLM outputs
        3. Filter low-quality examples
        4. Add metadata (candidate info, recruiter feedback)
        5. Store in training_data table
        6. Mark for human review
        """
        
        # Download from batch server
        results = await self.batch_client.get_results(batch_id)
        
        # Parse and filter
        for result in results:
            llm_output = result['response']['body']['choices'][0]['message']['content']
            candidate_id = result['custom_id']
            
            # Get candidate context
            candidate = await self.db.get_candidate(candidate_id)
            
            # Quality filter
            if self.is_high_quality(llm_output, candidate):
                # Store as training example
                await self.db.training_data.insert({
                    'candidate_id': candidate_id,
                    'llm_output': llm_output,
                    'candidate_context': candidate.to_dict(),
                    'model': result['model'],
                    'created_at': datetime.now(),
                    'quality_score': self.calculate_quality(llm_output),
                    'needs_review': True,  # Human-in-the-loop
                    'batch_id': batch_id
                })
    
    async def export_for_finetuning(self, min_quality: float = 0.8):
        """
        Export curated training data for fine-tuning.
        """
        examples = await self.db.training_data.find({
            'quality_score': {'$gte': min_quality},
            'needs_review': False,  # Already reviewed
            'approved': True
        })
        
        # Convert to fine-tuning format
        return self.format_for_finetuning(examples)
```

---

## ğŸ“‹ **Data Flow: End-to-End**

### **Step 1: Submit Batch (Aris â†’ Batch Server)**

```python
# In Aris web app
batch_id = await batch_client.submit_batch(
    file='candidates_170k.jsonl',
    model='google/gemma-3-4b-it'
)
```

### **Step 2: Process (Batch Server)**

```
Batch Server:
1. Queue job
2. Worker picks up job
3. Chunk into 5K pieces
4. Run vLLM inference
5. Save results incrementally
6. Mark as completed
```

### **Step 3: Download Results (Aris)**

```python
# Poll for completion
while True:
    status = await batch_client.get_status(batch_id)
    if status['status'] == 'completed':
        break
    await asyncio.sleep(10)

# Download results
results = await batch_client.get_results(batch_id)
```

### **Step 4: Curate Training Data (Aris)**

```python
# In Aris web app
await training_data_service.create_training_data_from_batch(batch_id)

# This:
# - Parses LLM outputs
# - Filters low-quality
# - Adds candidate metadata
# - Stores in permanent DB
# - Marks for human review
```

### **Step 5: Human Review (Aris UI)**

```
Recruiter reviews training examples:
- âœ… Approve good examples
- âŒ Reject bad examples
- âœï¸ Edit/improve examples
- ğŸ·ï¸ Add labels (positive/negative)
```

### **Step 6: Export for Fine-Tuning (Aris)**

```python
# Export approved training data
training_data = await training_data_service.export_for_finetuning(
    min_quality=0.8,
    approved_only=True
)

# Format: OpenAI fine-tuning JSONL
# {"messages": [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
```

---

## ğŸŒ **Web-Based Instructions for Agents**

### **4. How do agents know how to interact?**

**Answer: OpenAPI/Swagger Documentation + Health Page**

#### **Option A: OpenAPI Spec (Recommended)**

```python
# batch_app/api_server.py

from fastapi import FastAPI
from fastapi.openapi.utils import get_openapi

app = FastAPI(
    title="vLLM Batch Processing API",
    description="Enterprise-grade batch inference service for LLMs",
    version="1.0.0",
    docs_url="/docs",  # Swagger UI
    redoc_url="/redoc"  # ReDoc
)

# Agents can access:
# http://localhost:4080/docs - Interactive API docs
# http://localhost:4080/openapi.json - Machine-readable spec
```

#### **Option B: Enhanced Health Page**

Let me create an interactive health page with API instructions...

---

## ğŸ“Š **Storage Recommendations**

### **Batch Server (Temporary)**

```yaml
Storage:
  Input Files: 7 days
  Output Files: 30 days
  Failed Requests: 30 days
  Job Metadata: Permanent (for analytics)
  
Database:
  SQLite (current) - Good for single server
  PostgreSQL (future) - Better for multi-server
  
Backup:
  Not critical (temporary data)
  Web app should download within 30 days
```

### **Web App (Permanent)**

```yaml
Training Data Storage:
  Database: PostgreSQL (ACID compliance)
  Backup: Daily (critical business asset)
  Retention: Permanent
  Version Control: Track changes to examples
  
Schema:
  training_examples:
    - id
    - candidate_id
    - llm_output
    - candidate_context (JSON)
    - quality_score
    - approved (boolean)
    - reviewed_by (user_id)
    - created_at
    - updated_at
    - batch_id (reference to batch server)
```

---

## ğŸš€ **Next Steps**

1. âœ… **Keep batch server simple** - Just inference
2. âœ… **Build training data service in Aris** - Business logic
3. âœ… **Add retention policy** - Auto-delete old batch results
4. âœ… **Create OpenAPI docs** - For agent integration
5. âœ… **Build human review UI** - Quality control
6. âœ… **Export pipeline** - Fine-tuning format

---

## ğŸ“ **Summary**

**Best Practice: Separation of Concerns**

| Component | Responsibility | Storage | Retention |
|-----------|---------------|---------|-----------|
| **Batch Server** | Inference only | Temporary (30 days) | Auto-delete |
| **Web App** | Training data curation | Permanent | Forever |
| **Human Review** | Quality control | In web app | Forever |
| **Fine-Tuning Export** | Format conversion | In web app | Forever |

**This is the enterprise way:** Clean separation, clear responsibilities, scalable architecture! ğŸ¯

