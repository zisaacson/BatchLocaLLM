# üéØ Non-Technical User Guide: Adding New Models

## Overview

This guide walks you through adding a new AI model to your vLLM Batch Server **without any technical knowledge**. The system handles all the complexity for you - you just need to copy and paste!

---

## ‚ú® What You'll Accomplish

By the end of this guide, you'll be able to:
1. Find models on HuggingFace
2. Add them to your system with one click
3. See how they perform compared to existing models
4. Use them for batch processing

**Time Required:** 5-10 minutes per model

---

## üìã Step-by-Step Instructions

### Step 1: Find a Model on HuggingFace

1. Go to [HuggingFace Models](https://huggingface.co/models)
2. Search for models that interest you (e.g., "OLMo 2 7B", "Gemma 3 12B", "Llama 3.2")
3. Look for models with "GGUF" in the name (these are optimized for consumer GPUs)
4. Click on a model to open its page

**Recommended Models for RTX 4080 16GB:**
- `bartowski/OLMo-2-1124-7B-Instruct-GGUF` - Great quality, good speed
- `bartowski/Llama-3.2-3B-Instruct-GGUF` - Fast, decent quality
- `bartowski/Qwen3-4B-Instruct-GGUF` - Balanced speed and quality

### Step 2: Copy the Model URL

On the HuggingFace model page:
1. Look at the URL in your browser's address bar
2. Copy the entire URL (e.g., `https://huggingface.co/bartowski/OLMo-2-1124-7B-Instruct-GGUF`)

**That's it!** You don't need to copy anything else from the page.

### Step 3: Open the Add Model Page

1. Open your browser
2. Go to: `http://localhost:4080/add-model`
3. You'll see a simple form with a text box

### Step 4: Paste and Analyze

1. Paste the URL you copied into the text box
2. Click the **"Analyze Model"** button
3. Wait 2-3 seconds while the system analyzes the model

### Step 5: Review the Analysis

The system will show you:

**‚úÖ Will It Work?**
- Green badge = Model will work on your GPU
- Yellow badge = Model needs CPU offload (slower but works)
- Red badge = Model won't fit (try a smaller version)

**üìä Memory Requirements**
- How much GPU memory the model needs
- Whether CPU offload is required
- How much memory is available

**‚ö° Performance Estimate**
- Estimated speed (tokens per second)
- Quality rating (1-5 stars)
- Comparison to your existing models

**Example Analysis:**
```
‚úÖ Compatible with RTX 4080

Memory:
- Model: 7.0 GB
- KV Cache: 2.87 GB
- Overhead: 2.35 GB
- Total: 12.22 GB
- Available: 16.0 GB
- CPU Offload: 0 GB (fits entirely on GPU!)

Performance:
- Estimated Speed: 700 tokens/sec
- Quality: ‚≠ê‚≠ê‚≠ê‚≠ê (4/5 stars)
- 2.8x faster than Gemma 3 4B
```

### Step 6: Download and Install

1. If the analysis looks good, click **"Download & Benchmark"**
2. Watch the progress bar as the system:
   - Downloads the model from HuggingFace
   - Installs it to your local system
   - Runs a quick benchmark test
   - Adds it to your model registry

**This can take 5-30 minutes depending on:**
- Model size (larger = longer)
- Your internet speed
- Your computer's speed

### Step 7: Use Your New Model

Once installation is complete:

1. Go to the job history page: `http://localhost:4080/history`
2. Create a new batch job
3. Select your new model from the dropdown
4. Submit your batch!

---

## üéì Understanding the Results

### Memory Breakdown

**Model Size:** The actual AI model weights
- 4B models: ~4 GB
- 7B models: ~7 GB
- 12B models: ~12 GB

**KV Cache:** Memory for processing multiple requests at once
- Scales with batch size and context length
- Usually 2-4 GB for typical workloads

**Overhead:** vLLM engine memory
- Usually 2-3 GB
- Required for the inference engine to run

**CPU Offload:** When model doesn't fit entirely on GPU
- System moves some model weights to RAM
- Slower but allows running larger models
- 0 GB = best performance (everything on GPU)
- 5+ GB = noticeable slowdown

### Performance Metrics

**Tokens Per Second:** How fast the model generates text
- 2000+ tok/s = Very fast (small models)
- 500-1000 tok/s = Fast (medium models)
- 100-500 tok/s = Moderate (large models)
- <100 tok/s = Slow (very large models or heavy CPU offload)

**Quality Stars:** Rough estimate of output quality
- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5 stars) = Excellent (20B+ models)
- ‚≠ê‚≠ê‚≠ê‚≠ê (4 stars) = Very good (7-12B models)
- ‚≠ê‚≠ê‚≠ê (3 stars) = Good (3-4B models)
- ‚≠ê‚≠ê (2 stars) = Decent (1-2B models)
- ‚≠ê (1 star) = Basic (<1B models)

---

## üö® Troubleshooting

### "Could not extract model ID"

**Problem:** The system couldn't understand what you pasted.

**Solution:**
1. Make sure you copied the full URL from HuggingFace
2. URL should look like: `https://huggingface.co/organization/model-name`
3. Try copying just the URL, not the entire page content

### "Model won't fit on GPU"

**Problem:** The model is too large for your RTX 4080 16GB.

**Solution:**
1. Look for a smaller version (e.g., 4B instead of 7B)
2. Look for quantized versions (Q4_0, Q8_0 in the name)
3. Try models from the "Recommended Models" list above

### "Download failed"

**Problem:** Network issue or HuggingFace is down.

**Solution:**
1. Check your internet connection
2. Try again in a few minutes
3. Make sure you have enough disk space (models can be 5-20 GB)

### "Benchmark taking too long"

**Problem:** Benchmark is running but seems stuck.

**Solution:**
1. Be patient - first run can take 10-15 minutes
2. Check the progress bar for updates
3. If stuck for >30 minutes, refresh the page and try again

---

## üí° Tips for Success

### Choosing the Right Model

**For Speed:** Choose smaller models (3-4B parameters)
- Llama 3.2 3B
- Qwen 3 4B
- Gemma 3 4B

**For Quality:** Choose larger models (7-12B parameters)
- OLMo 2 7B
- Llama 3.2 7B (if quantized)
- Gemma 3 12B (if quantized)

**For Balance:** Choose 4-7B models with good quantization
- OLMo 2 7B Q4_0
- Qwen 3 4B
- Gemma 3 4B

### Understanding Quantization

Models with these in the name are optimized for consumer GPUs:
- **Q4_0** = 4-bit quantization (smallest, fastest)
- **Q8_0** = 8-bit quantization (balanced)
- **GGUF** = Optimized format for llama.cpp/vLLM

**Rule of thumb:** Q4_0 models are ~4x smaller than original models!

### Disk Space Management

Models take up disk space:
- 3-4B models: 2-4 GB
- 7B models: 4-8 GB
- 12B models: 6-12 GB

**Check your disk space before downloading large models!**

---

## üéØ Next Steps

After adding your first model:

1. **Compare Models:** Run the same batch on multiple models to see quality differences
2. **Benchmark Properly:** Run a full benchmark with 1000+ samples for accurate speed metrics
3. **Optimize Settings:** Adjust batch size and context length for your use case
4. **Share Results:** Export your benchmark results to share with the community

---

## üìö Additional Resources

- **HuggingFace Model Hub:** https://huggingface.co/models
- **vLLM Documentation:** https://docs.vllm.ai
- **GGUF Format Guide:** https://github.com/ggerganov/llama.cpp

---

## ‚úÖ Summary

**What you learned:**
1. How to find models on HuggingFace
2. How to analyze if a model will work on your GPU
3. How to install models with one click
4. How to understand performance metrics
5. How to troubleshoot common issues

**You're now ready to add any model to your vLLM Batch Server!** üöÄ

