"""
Integration Tests for Unified Workbench

Tests the complete end-to-end workflow:
1. Upload dataset
2. Run benchmark on dataset
3. Check progress
4. Get results
5. Annotate results (golden/fix/wrong)
6. Export golden dataset

These are REAL integration tests that test actual data flows,
not just mocked unit tests.

NOTE: These tests require the API server to be running:
    python -m core.batch_app.api_server
"""

import json
import time
import pytest
import requests
from pathlib import Path
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from core.batch_app.database import Dataset, Benchmark, Annotation, ModelRegistry, Base

# Test configuration
API_BASE = "http://localhost:4080/admin"
DB_URL = "sqlite:///data/batch_server.db"

# Create database session
engine = create_engine(DB_URL)
SessionLocal = sessionmaker(bind=engine)


def check_server_running():
    """Check if API server is running."""
    try:
        response = requests.get("http://localhost:4080/health", timeout=2)
        return response.status_code == 200
    except:
        return False


@pytest.fixture(scope="session", autouse=True)
def ensure_server_running():
    """Ensure server is running before tests."""
    if not check_server_running():
        pytest.skip("API server not running. Start with: python -m core.batch_app.api_server")


@pytest.fixture(scope="session", autouse=True)
def ensure_tables_exist():
    """Ensure database tables exist."""
    Base.metadata.create_all(bind=engine)


@pytest.fixture
def test_dataset_file(tmp_path):
    """Create a small test dataset (10 requests)."""
    dataset_path = tmp_path / "test_dataset.jsonl"
    
    requests_data = []
    for i in range(10):
        requests_data.append({
            "custom_id": f"test-candidate-{i}",
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": "test-model",
                "messages": [
                    {
                        "role": "system",
                        "content": "You are a helpful assistant."
                    },
                    {
                        "role": "user",
                        "content": f"Evaluate candidate {i}:\nName: Test Candidate {i}\nTitle: Software Engineer"
                    }
                ]
            }
        })
    
    with open(dataset_path, "w") as f:
        for req in requests_data:
            f.write(json.dumps(req) + "\n")
    
    return dataset_path


@pytest.fixture
def cleanup_test_data():
    """Clean up test data after tests."""
    yield
    
    # Clean up database
    db = SessionLocal()
    try:
        # Delete test datasets
        db.query(Dataset).filter(Dataset.name.like("test_%")).delete()
        # Delete test benchmarks
        db.query(Benchmark).filter(Benchmark.id.like("test_%")).delete()
        # Delete test annotations
        db.query(Annotation).filter(Annotation.dataset_id.like("test_%")).delete()
        db.commit()
    finally:
        db.close()


class TestDatasetManagement:
    """Test dataset upload and management."""
    
    def test_upload_dataset(self, test_dataset_file, cleanup_test_data):
        """Test uploading a dataset via API."""
        with open(test_dataset_file, "rb") as f:
            files = {"file": (test_dataset_file.name, f, "application/json")}
            response = requests.post(f"{API_BASE}/datasets/upload", files=files)
        
        assert response.status_code == 200
        data = response.json()
        
        assert "dataset_id" in data
        assert data["name"] == test_dataset_file.name
        assert data["count"] == 10
        
        # Verify in database
        db = SessionLocal()
        try:
            dataset = db.query(Dataset).filter(Dataset.id == data["dataset_id"]).first()
            assert dataset is not None
            assert dataset.name == test_dataset_file.name
            assert dataset.count == 10
            assert Path(dataset.file_path).exists()
        finally:
            db.close()
    
    def test_list_datasets(self, test_dataset_file, cleanup_test_data):
        """Test listing datasets."""
        # Upload a dataset first
        with open(test_dataset_file, "rb") as f:
            files = {"file": (test_dataset_file.name, f, "application/json")}
            upload_response = requests.post(f"{API_BASE}/datasets/upload", files=files)
        
        assert upload_response.status_code == 200
        dataset_id = upload_response.json()["dataset_id"]
        
        # List datasets
        response = requests.get(f"{API_BASE}/datasets")
        assert response.status_code == 200
        
        data = response.json()
        assert "datasets" in data
        
        # Find our test dataset
        test_datasets = [d for d in data["datasets"] if d["id"] == dataset_id]
        assert len(test_datasets) == 1
        assert test_datasets[0]["count"] == 10
    
    def test_delete_dataset(self, test_dataset_file, cleanup_test_data):
        """Test deleting a dataset."""
        # Upload a dataset first
        with open(test_dataset_file, "rb") as f:
            files = {"file": (test_dataset_file.name, f, "application/json")}
            upload_response = requests.post(f"{API_BASE}/datasets/upload", files=files)
        
        dataset_id = upload_response.json()["dataset_id"]
        
        # Delete it
        response = requests.delete(f"{API_BASE}/datasets/{dataset_id}")
        assert response.status_code == 200
        
        # Verify it's gone from database
        db = SessionLocal()
        try:
            dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()
            assert dataset is None
        finally:
            db.close()


class TestBenchmarkWorkflow:
    """Test the benchmark workflow (without actually running vLLM)."""
    
    def test_benchmark_creation(self, test_dataset_file, cleanup_test_data):
        """Test creating a benchmark run."""
        # Upload dataset
        with open(test_dataset_file, "rb") as f:
            files = {"file": (test_dataset_file.name, f, "application/json")}
            upload_response = requests.post(f"{API_BASE}/datasets/upload", files=files)
        
        dataset_id = upload_response.json()["dataset_id"]
        
        # Get a model from registry
        models_response = requests.get(f"{API_BASE}/models")
        assert models_response.status_code == 200
        models = models_response.json()["models"]
        
        if not models:
            pytest.skip("No models in registry to test with")
        
        model_id = models[0]["model_id"]
        
        # Create benchmark (this will fail if vLLM not available, but we test the API)
        response = requests.post(
            f"{API_BASE}/benchmarks/run",
            json={
                "model_id": model_id,
                "dataset_id": dataset_id
            }
        )
        
        # Should create benchmark record even if execution fails
        assert response.status_code in [200, 500]  # 500 if vLLM not available
        
        if response.status_code == 200:
            data = response.json()
            assert "benchmark_id" in data
            
            # Verify in database
            db = SessionLocal()
            try:
                benchmark = db.query(Benchmark).filter(Benchmark.id == data["benchmark_id"]).first()
                assert benchmark is not None
                assert benchmark.model_id == model_id
                assert benchmark.dataset_id == dataset_id
                assert benchmark.total == 10
            finally:
                db.close()
    
    def test_list_active_benchmarks(self, cleanup_test_data):
        """Test listing active benchmarks."""
        response = requests.get(f"{API_BASE}/benchmarks/active")
        assert response.status_code == 200
        
        data = response.json()
        assert "jobs" in data
        assert isinstance(data["jobs"], list)


class TestAnnotationWorkflow:
    """Test the annotation workflow."""
    
    def test_toggle_golden(self, test_dataset_file, cleanup_test_data):
        """Test marking a result as golden."""
        # Upload dataset
        with open(test_dataset_file, "rb") as f:
            files = {"file": (test_dataset_file.name, f, "application/json")}
            upload_response = requests.post(f"{API_BASE}/datasets/upload", files=files)
        
        dataset_id = upload_response.json()["dataset_id"]
        
        # Get a model
        models_response = requests.get(f"{API_BASE}/models")
        models = models_response.json()["models"]
        
        if not models:
            pytest.skip("No models in registry to test with")
        
        model_id = models[0]["model_id"]
        candidate_id = "test-candidate-0"
        
        # Mark as golden
        response = requests.post(
            f"{API_BASE}/annotations/golden/{dataset_id}/{candidate_id}",
            params={"model_id": model_id}
        )
        
        assert response.status_code == 200
        data = response.json()
        assert data["is_golden"] is True
        
        # Verify in database
        db = SessionLocal()
        try:
            annotation = db.query(Annotation).filter(
                Annotation.dataset_id == dataset_id,
                Annotation.candidate_id == candidate_id,
                Annotation.model_id == model_id
            ).first()
            assert annotation is not None
            assert annotation.is_golden is True
        finally:
            db.close()
        
        # Toggle off
        response = requests.post(
            f"{API_BASE}/annotations/golden/{dataset_id}/{candidate_id}",
            params={"model_id": model_id}
        )
        
        assert response.status_code == 200
        data = response.json()
        assert data["is_golden"] is False
    
    def test_mark_fixed(self, test_dataset_file, cleanup_test_data):
        """Test marking a result as fixed."""
        # Upload dataset
        with open(test_dataset_file, "rb") as f:
            files = {"file": (test_dataset_file.name, f, "application/json")}
            upload_response = requests.post(f"{API_BASE}/datasets/upload", files=files)
        
        dataset_id = upload_response.json()["dataset_id"]
        
        # Get a model
        models_response = requests.get(f"{API_BASE}/models")
        models = models_response.json()["models"]
        
        if not models:
            pytest.skip("No models in registry to test with")
        
        model_id = models[0]["model_id"]
        candidate_id = "test-candidate-1"
        
        # Mark as fixed
        response = requests.post(
            f"{API_BASE}/annotations/fix/{dataset_id}/{candidate_id}",
            params={"model_id": model_id}
        )
        
        assert response.status_code == 200
        
        # Verify in database
        db = SessionLocal()
        try:
            annotation = db.query(Annotation).filter(
                Annotation.dataset_id == dataset_id,
                Annotation.candidate_id == candidate_id
            ).first()
            assert annotation is not None
            assert annotation.is_fixed is True
        finally:
            db.close()


class TestEndToEndWorkflow:
    """Test the complete end-to-end workflow."""
    
    def test_complete_workflow(self, test_dataset_file, cleanup_test_data):
        """
        Test complete workflow:
        1. Upload dataset
        2. List datasets
        3. Get models
        4. Annotate a result
        5. Get results with annotations
        """
        # 1. Upload dataset
        with open(test_dataset_file, "rb") as f:
            files = {"file": (test_dataset_file.name, f, "application/json")}
            upload_response = requests.post(f"{API_BASE}/datasets/upload", files=files)
        
        assert upload_response.status_code == 200
        dataset_id = upload_response.json()["dataset_id"]
        print(f"âœ… Step 1: Uploaded dataset {dataset_id}")
        
        # 2. List datasets
        list_response = requests.get(f"{API_BASE}/datasets")
        assert list_response.status_code == 200
        datasets = list_response.json()["datasets"]
        assert any(d["id"] == dataset_id for d in datasets)
        print(f"âœ… Step 2: Dataset appears in list")
        
        # 3. Get models
        models_response = requests.get(f"{API_BASE}/models")
        assert models_response.status_code == 200
        models = models_response.json()["models"]
        
        if not models:
            pytest.skip("No models in registry to test with")
        
        model_id = models[0]["model_id"]
        print(f"âœ… Step 3: Found model {model_id}")
        
        # 4. Annotate a result
        candidate_id = "test-candidate-0"
        annotation_response = requests.post(
            f"{API_BASE}/annotations/golden/{dataset_id}/{candidate_id}",
            params={"model_id": model_id}
        )
        assert annotation_response.status_code == 200
        print(f"âœ… Step 4: Marked candidate as golden")
        
        # 5. Verify annotation in database
        db = SessionLocal()
        try:
            annotation = db.query(Annotation).filter(
                Annotation.dataset_id == dataset_id,
                Annotation.candidate_id == candidate_id
            ).first()
            assert annotation is not None
            assert annotation.is_golden is True
            print(f"âœ… Step 5: Annotation saved to database")
        finally:
            db.close()
        
        print("\nðŸŽ‰ COMPLETE END-TO-END WORKFLOW TEST PASSED!")


if __name__ == "__main__":
    print("=" * 80)
    print("ðŸ§ª UNIFIED WORKBENCH - INTEGRATION TESTS")
    print("=" * 80)
    print()
    print("These tests verify the complete data flow:")
    print("- Dataset upload â†’ Database â†’ API")
    print("- Benchmark creation â†’ Background process â†’ Progress tracking")
    print("- Annotation â†’ Database â†’ Label Studio")
    print()
    
    # Run tests
    pytest.main([__file__, "-v", "-s"])

