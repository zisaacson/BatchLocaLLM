# Web App + vLLM Offline Batch Processing Architecture

**Goal:** Accept 200K batch requests via web API, process with vLLM Offline, return results.

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER / WEB APP                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ POST /v1/batches
                              â”‚ (Upload JSONL file)
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      BATCH API SERVER                           â”‚
â”‚                     (FastAPI / Flask)                           â”‚
â”‚                                                                 â”‚
â”‚  â€¢ Accept batch job requests                                    â”‚
â”‚  â€¢ Validate input files                                         â”‚
â”‚  â€¢ Generate batch_id                                            â”‚
â”‚  â€¢ Queue job in database                                        â”‚
â”‚  â€¢ Return batch_id to user                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ Write to queue
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      JOB QUEUE / DATABASE                       â”‚
â”‚                     (SQLite / PostgreSQL)                       â”‚
â”‚                                                                 â”‚
â”‚  batch_id | status   | input_file | output_file | created_at   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  abc-123  | pending  | batch.jsonl| results.jsonl| 2025-10-28  â”‚
â”‚  def-456  | running  | batch2.jsonl| ...         | 2025-10-28  â”‚
â”‚  ghi-789  | completed| batch3.jsonl| done.jsonl  | 2025-10-27  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ Poll for pending jobs
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      BATCH WORKER                               â”‚
â”‚                   (Background Process)                          â”‚
â”‚                                                                 â”‚
â”‚  while True:                                                    â”‚
â”‚    job = get_next_pending_job()                                 â”‚
â”‚    if job:                                                      â”‚
â”‚      run_vllm_offline(job)                                      â”‚
â”‚      update_status(job, "completed")                            â”‚
â”‚    sleep(10)                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ Runs vLLM Offline
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      vLLM OFFLINE ENGINE                        â”‚
â”‚                                                                 â”‚
â”‚  from vllm import LLM, SamplingParams                           â”‚
â”‚  llm = LLM(model="google/gemma-3-4b-it")                        â”‚
â”‚  outputs = llm.generate(prompts, sampling_params)               â”‚
â”‚                                                                 â”‚
â”‚  â€¢ Processes 200K requests                                      â”‚
â”‚  â€¢ Takes ~24 hours                                              â”‚
â”‚  â€¢ Saves results to file                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ Results saved
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RESULTS STORAGE                            â”‚
â”‚                   (File System / S3)                            â”‚
â”‚                                                                 â”‚
â”‚  /results/abc-123/output.jsonl                                  â”‚
â”‚  /results/def-456/output.jsonl                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ GET /v1/batches/{batch_id}
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER / WEB APP                          â”‚
â”‚                                                                 â”‚
â”‚  â€¢ Poll for status                                              â”‚
â”‚  â€¢ Download results when complete                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ API Endpoints

### **1. Submit Batch Job**
```http
POST /v1/batches
Content-Type: multipart/form-data

{
  "file": <batch.jsonl>,
  "model": "google/gemma-3-4b-it"
}
```

**Response:**
```json
{
  "batch_id": "batch_abc123",
  "status": "pending",
  "created_at": "2025-10-28T10:00:00Z",
  "estimated_completion": "2025-10-29T10:00:00Z"
}
```

---

### **2. Check Batch Status**
```http
GET /v1/batches/{batch_id}
```

**Response (Pending):**
```json
{
  "batch_id": "batch_abc123",
  "status": "pending",
  "progress": {
    "total": 200000,
    "completed": 0,
    "failed": 0
  },
  "created_at": "2025-10-28T10:00:00Z"
}
```

**Response (Running):**
```json
{
  "batch_id": "batch_abc123",
  "status": "running",
  "progress": {
    "total": 200000,
    "completed": 50000,
    "failed": 0,
    "percent": 25.0
  },
  "started_at": "2025-10-28T10:05:00Z",
  "estimated_completion": "2025-10-29T10:00:00Z"
}
```

**Response (Completed):**
```json
{
  "batch_id": "batch_abc123",
  "status": "completed",
  "progress": {
    "total": 200000,
    "completed": 200000,
    "failed": 0
  },
  "started_at": "2025-10-28T10:05:00Z",
  "completed_at": "2025-10-29T10:15:00Z",
  "results_url": "/v1/batches/batch_abc123/results"
}
```

---

### **3. Download Results**
```http
GET /v1/batches/{batch_id}/results
```

**Response:**
```
Content-Type: application/x-ndjson
Content-Disposition: attachment; filename="batch_abc123_results.jsonl"

{"custom_id": "req-1", "response": {...}}
{"custom_id": "req-2", "response": {...}}
...
```

---

### **4. List All Batches**
```http
GET /v1/batches?status=completed&limit=10
```

**Response:**
```json
{
  "batches": [
    {
      "batch_id": "batch_abc123",
      "status": "completed",
      "created_at": "2025-10-28T10:00:00Z"
    },
    {
      "batch_id": "batch_def456",
      "status": "running",
      "created_at": "2025-10-28T11:00:00Z"
    }
  ]
}
```

---

## ğŸ”§ Implementation Components

### **Component 1: Batch API Server** (`batch_api_server.py`)
- FastAPI web server
- Handles HTTP requests
- Validates input files
- Manages job queue
- Serves results

### **Component 2: Batch Worker** (`batch_worker.py`)
- Background process
- Polls database for pending jobs
- Runs vLLM Offline
- Updates job status
- Handles errors

### **Component 3: Database** (`batch_jobs.db`)
- SQLite or PostgreSQL
- Stores job metadata
- Tracks progress
- Manages queue

### **Component 4: File Storage**
- Input files: `/data/batches/input/`
- Output files: `/data/batches/output/`
- Logs: `/data/batches/logs/`

---

## ğŸš€ How It Works

### **User Workflow:**

1. **User uploads batch file** (200K requests)
   ```bash
   curl -X POST http://localhost:8080/v1/batches \
     -F "file=@batch_200k.jsonl" \
     -F "model=google/gemma-3-4b-it"
   ```

2. **Server returns batch_id**
   ```json
   {"batch_id": "batch_abc123", "status": "pending"}
   ```

3. **User polls for status** (every 60 seconds)
   ```bash
   curl http://localhost:8080/v1/batches/batch_abc123
   ```

4. **Worker picks up job** (background)
   - Loads vLLM model
   - Processes 200K requests
   - Saves results
   - Updates status to "completed"

5. **User downloads results**
   ```bash
   curl http://localhost:8080/v1/batches/batch_abc123/results > results.jsonl
   ```

---

## âš™ï¸ Technical Details

### **Concurrency Model:**
- **API Server:** Async (handles many users)
- **Worker:** Single-threaded (one batch at a time)
- **GPU:** Dedicated to worker process

### **Queue Management:**
- **FIFO:** First-in, first-out
- **Priority:** Optional (premium users first)
- **Retry:** Failed jobs can be retried

### **Progress Tracking:**
- Worker updates database every 1,000 requests
- API server reads from database
- Real-time progress available

### **Error Handling:**
- **Validation errors:** Return 400 immediately
- **Processing errors:** Mark job as "failed", save error log
- **GPU OOM:** Retry with smaller batch size

---

## ğŸ“Š Performance Estimates

| Batch Size | Processing Time | Queue Wait | Total Time |
|------------|-----------------|------------|------------|
| 5K | 37 min | 0-10 min | ~47 min |
| 50K | 6.1 hours | 0-30 min | ~6.5 hours |
| 200K | 24.5 hours | 0-2 hours | ~26 hours |

**Assumptions:**
- Single RTX 4080 16GB
- Gemma 3 4B model
- 2,511 tok/s throughput
- No other jobs in queue

---

## ğŸ” Security Considerations

1. **Authentication:** Require API keys
2. **Rate Limiting:** Max 10 batches per user per day
3. **File Size Limits:** Max 500 MB upload
4. **Input Validation:** Sanitize JSONL files
5. **Resource Limits:** Max 200K requests per batch

---

## ğŸ¯ Advantages of This Architecture

âœ… **Scalable:** Add more workers/GPUs as needed
âœ… **Reliable:** Jobs survive server restarts
âœ… **User-Friendly:** Simple HTTP API
âœ… **Efficient:** Uses vLLM Offline (proven to work)
âœ… **Flexible:** Can add real-time API later

---

## ğŸš§ Next Steps

1. **Build Batch API Server** (FastAPI)
2. **Build Batch Worker** (Python script)
3. **Set up Database** (SQLite for MVP)
4. **Test with 5K batch**
5. **Scale to 200K**

---

## ğŸ’¡ Alternative: Hybrid Mode

**Can you run BOTH batch processing AND real-time API?**

**Option 1: Time-sharing**
- 9am-5pm: Real-time API (vLLM Serve)
- 5pm-9am: Batch processing (vLLM Offline)

**Option 2: Multi-GPU**
- GPU 1: Real-time API (always on)
- GPU 2: Batch processing (queue)

**Option 3: Dynamic switching**
- If no batch jobs: Run vLLM Serve
- If batch job arrives: Stop Serve, run Offline, restart Serve

---

## ğŸ“ Summary

**YES, you can build a web app that:**
1. Accepts 200K batch requests
2. Runs vLLM Offline in background
3. Returns results when complete

**This is exactly how ALCF's system works!**

**Want me to build the MVP?**

