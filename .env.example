# =============================================================================
# vLLM Batch Server Configuration
# =============================================================================

# -----------------------------------------------------------------------------
# Model Configuration
# -----------------------------------------------------------------------------

# Hugging Face model name or path
MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct

# Model revision (branch/tag/commit)
MODEL_REVISION=main

# Hugging Face token (required for gated models like Llama)
# Get yours at: https://huggingface.co/settings/tokens
HF_TOKEN=

# Trust remote code (required for some models)
TRUST_REMOTE_CODE=false

# -----------------------------------------------------------------------------
# GPU Configuration
# -----------------------------------------------------------------------------

# Number of GPUs to use for tensor parallelism
# Set to 1 for single GPU (RTX 4080)
# Set to 2+ for multi-GPU setups
TENSOR_PARALLEL_SIZE=1

# GPU memory utilization (0.0 to 1.0)
# 0.9 = use 90% of GPU memory, leave 10% for CUDA overhead
GPU_MEMORY_UTILIZATION=0.9

# Maximum model length (context window)
# Llama 3.1: 128K, but set lower to save memory
MAX_MODEL_LEN=8192

# Data type for model weights
# Options: auto, float16, bfloat16, float32
DTYPE=auto

# Quantization method (optional)
# Options: awq, gptq, squeezellm, fp8
# Leave empty for no quantization
QUANTIZATION=

# -----------------------------------------------------------------------------
# Batch Processing Configuration
# -----------------------------------------------------------------------------

# Maximum number of sequences to process in a single batch
MAX_NUM_SEQS=256

# Maximum number of concurrent batch jobs
MAX_CONCURRENT_BATCHES=4

# Maximum batch file size in MB
MAX_BATCH_FILE_SIZE_MB=500

# Maximum number of requests per batch
MAX_REQUESTS_PER_BATCH=50000

# Batch completion timeout (hours)
BATCH_COMPLETION_TIMEOUT_HOURS=24

# -----------------------------------------------------------------------------
# Prefix Caching Configuration
# -----------------------------------------------------------------------------

# Enable automatic prefix caching
# This caches common prompt prefixes (system messages, examples)
# Can reduce prompt processing time by 80%+
ENABLE_PREFIX_CACHING=true

# Number of GPU blocks to reserve for prefix caching
# Higher = more cache, but less memory for active requests
# Set to 0.1 to reserve 10% of KV cache for prefixes
PREFIX_CACHE_RATIO=0.1

# -----------------------------------------------------------------------------
# Server Configuration
# -----------------------------------------------------------------------------

# Server host
HOST=0.0.0.0

# Server port
PORT=8000

# API key for authentication (optional)
# Leave empty to disable authentication
API_KEY=

# Enable CORS
ENABLE_CORS=true

# Allowed CORS origins (comma-separated)
CORS_ORIGINS=*

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Enable structured JSON logging
JSON_LOGGING=true

# -----------------------------------------------------------------------------
# Storage Configuration
# -----------------------------------------------------------------------------

# Path to store batch jobs and results
# This will be mounted as a Docker volume
STORAGE_PATH=/data/batches

# Database path for job metadata
DATABASE_PATH=/data/vllm_batch.db

# Cleanup completed jobs after N days
CLEANUP_AFTER_DAYS=7

# -----------------------------------------------------------------------------
# Performance Tuning
# -----------------------------------------------------------------------------

# Enable CUDA graph optimization
# Reduces kernel launch overhead
ENABLE_CUDA_GRAPH=true

# Swap space for offloading (GB)
# Set to 0 to disable CPU offloading
SWAP_SPACE_GB=4

# Block size for paged attention
# Default: 16 (good for most cases)
BLOCK_SIZE=16

# Maximum number of parallel loading workers
MAX_PARALLEL_LOADING_WORKERS=4

# -----------------------------------------------------------------------------
# Monitoring & Observability
# -----------------------------------------------------------------------------

# Enable Prometheus metrics endpoint
ENABLE_METRICS=true

# Metrics port
METRICS_PORT=9090

# Enable health check endpoints
ENABLE_HEALTH_CHECKS=true

# -----------------------------------------------------------------------------
# Advanced vLLM Options
# -----------------------------------------------------------------------------

# Disable log stats (reduces logging verbosity)
DISABLE_LOG_STATS=false

# Disable log requests (reduces logging verbosity)
DISABLE_LOG_REQUESTS=false

# Enable chunked prefill
# Allows processing large prefills in chunks
ENABLE_CHUNKED_PREFILL=true

# Maximum number of batched tokens
MAX_NUM_BATCHED_TOKENS=8192

# Scheduler delay (seconds)
# Small delay to batch more requests together
SCHEDULER_DELAY=0.0

# -----------------------------------------------------------------------------
# Development & Debugging
# -----------------------------------------------------------------------------

# Enable debug mode
DEBUG=false

# Enable request/response logging
LOG_REQUESTS=false

# Enable detailed error messages
DETAILED_ERRORS=true

