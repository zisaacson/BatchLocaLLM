# üöÄ vLLM Batch Server - Benchmark Results & Testing Log

**Last Updated:** 2025-10-28

This is the **single source of truth** for all benchmark results, testing decisions, and performance comparisons across both `ollama` and `vllm` branches.

---

## üìä Current Status

### Active Testing
- **Branch:** `vllm`
- **Phase:** A/B Testing Setup
- **Goal:** Compare Offline vs Server mode performance with 5K sample dataset
- **Dataset:** `batch_5k.jsonl` (5,000 candidates)

### Completed Work
- ‚úÖ **Ollama Branch:** Complete implementation with benchmarking system
- ‚úÖ **vLLM Branch:** Code complete, dependencies updated to Python 3.13
- ‚è≥ **vLLM Testing:** In progress

---

## üéØ Test Plan: Offline vs Server Mode (5K Sample)

### Test Matrix

| Mode | Model | Status | Throughput | Total Time | VRAM |
|------|-------|--------|------------|------------|------|
| **Offline** | Gemma 12B 4-bit | ‚è≥ Pending | - | - | - |
| **Offline** | Mistral 7B | ‚è≥ Pending | - | - | - |
| **Offline** | Qwen 14B | ‚è≥ Pending | - | - | - |
| **Server** | Gemma 12B 4-bit | ‚è≥ Pending | - | - | - |
| **Server** | Mistral 7B | ‚è≥ Pending | - | - | - |
| **Server** | Qwen 14B | ‚è≥ Pending | - | - | - |

### Test Dataset
- **File:** `batch_5k.jsonl`
- **Size:** 5,000 requests
- **Source:** Sample from 170K candidate dataset
- **Format:** OpenAI Batch API format
- **Model (in file):** `gemma3:12b` (Ollama format - will be converted for vLLM)

---

## üìà Benchmark Results

### Ollama Branch (Consumer GPU - RTX 4080 16GB)

#### Model Comparison (Completed)

| Model | Size | Throughput | Avg Latency | VRAM Usage | Notes |
|-------|------|------------|-------------|------------|-------|
| gemma3:1b | 1B | 0.92 req/s | ~1.1s | ~4GB | ‚úÖ Fastest, fits easily |
| gemma3:4b | 4B | 0.45 req/s | ~2.2s | ~8GB | ‚úÖ Good balance |
| gemma3:12b | 12B | 0.22 req/s | ~4.5s | ~24GB | ‚ùå Doesn't fit in 16GB |

**Key Findings:**
- ‚úÖ **gemma3:1b** is 4.1x faster than 12B model
- ‚úÖ **73% GPU efficiency** achieved with 8 parallel workers
- ‚ùå **12B model doesn't fit** in RTX 4080 16GB VRAM (needs ~24GB)
- ‚è±Ô∏è **170K candidates:** ~54 hours with gemma3:1b, ~214 hours with gemma3:12b (if it fit)

**Configuration:**
- Workers: 8 parallel
- Batch size: Optimized per model
- Context window: 8K tokens
- Temperature: 0.7

---

### vLLM Branch (Production GPU - 24GB+ VRAM)

#### 4-bit Quantized Models (In Progress)

| Model | Size | Quantization | Throughput | Avg Latency | VRAM Usage | Notes |
|-------|------|--------------|------------|-------------|------------|-------|
| unsloth/gemma-3-12b-it-unsloth-bnb-4bit | 12B | 4-bit BnB | ‚è≥ Testing | ‚è≥ Testing | ~14GB | ‚úÖ Should fit in 16GB |
| mistralai/Mistral-7B-Instruct-v0.3 | 7B | FP16 | ‚è≥ Pending | ‚è≥ Pending | ~14GB | - |
| Qwen/Qwen2.5-14B-Instruct | 14B | FP16 | ‚è≥ Pending | ‚è≥ Pending | ~28GB | ‚ùå Won't fit in 16GB |

**Expected Results (Estimates):**
- üéØ **2-4x faster** than Ollama (vLLM's continuous batching)
- üéØ **0.5-1.0 req/s** for 12B 4-bit model
- üéØ **~47-94 hours** for 170K candidates (vs 214 hours Ollama)

**Configuration:**
- Engine: vLLM V1 (0.11.0)
- Python: 3.13.3
- GPU Memory Utilization: 0.9
- Max Model Length: 8192 tokens

---

## üî¨ Testing Methodology

### Phase 1: Offline Mode (vLLM LLM.generate)
**Purpose:** Establish baseline performance with vLLM's native batching

**Script:** `scripts/ab_test_offline.py`

**Process:**
1. Load model once
2. Process all 5K prompts in single batch
3. vLLM handles batching internally (continuous batching)
4. Measure: load time, inference time, throughput, VRAM

**Advantages:**
- Fastest (no HTTP overhead)
- Simplest (direct vLLM API)
- Optimal for pure batch workloads

---

### Phase 2: Server Mode (vLLM serve + HTTP API)
**Purpose:** Test production-ready API mode with OpenAI compatibility

**Script:** `scripts/ab_test_server.py`

**Process:**
1. Start vLLM server with model
2. Send 5K requests via HTTP (concurrent, no manual batching)
3. vLLM server handles batching automatically
4. Measure: throughput, latency, VRAM

**Advantages:**
- OpenAI API compatible
- Can monitor progress
- Can cancel/pause batches
- Production-ready

---

### Phase 3: Analysis & Decision
**Purpose:** Compare results and choose optimal approach

**Script:** `scripts/compare_ab_results.py`

**Decision Criteria:**
- Throughput (40% weight)
- Total time (30% weight)
- Quality (20% weight)
- Flexibility (10% weight)

---

## üéØ Key Decisions & Learnings

### Decision 1: Two Independent Branches
**Date:** 2025-10-28
**Decision:** Maintain separate `ollama` and `vllm` branches that never merge
**Reasoning:**
- Different hardware targets (16GB vs 24GB+ VRAM)
- Different use cases (consumer vs production)
- Different dependencies (Ollama vs vLLM)
- Cleaner to maintain separately

---

### Decision 2: Python 3.13 + Latest Dependencies
**Date:** 2025-10-28
**Decision:** Update both branches to Python 3.13.3 and latest dependencies
**Reasoning:**
- vLLM 0.11.0 works fine with Python 3.13
- Latest FastAPI, Pydantic, setuptools
- No need for conservative version pinning
- Future-proof

**Dependencies:**
- Python: 3.13.3
- vLLM: 0.11.0 (vllm branch)
- FastAPI: 0.120.0
- Pydantic: 2.12.3
- bitsandbytes: 0.46.1+ (vllm branch)

---

### Decision 3: Use Same 5K Dataset for All Tests
**Date:** 2025-10-28
**Decision:** Use `batch_5k.jsonl` for all A/B testing
**Reasoning:**
- Consistent comparison across models and modes
- Already generated and validated
- Representative sample of 170K dataset
- Fast enough for iterative testing

---

### Learning 1: vLLM Has Native Continuous Batching
**Date:** 2025-10-28
**Finding:** vLLM automatically batches requests - manual batching is redundant
**Impact:** 
- Don't need parallel worker pools
- Don't need manual batch chunking
- Just send requests, vLLM optimizes internally
**Action:** Use offline mode (`LLM.generate()`) or server mode (concurrent HTTP requests)

---

### Learning 2: 4-bit Quantization Enables 12B on 16GB
**Date:** 2025-10-28
**Finding:** 4-bit BitsAndBytes quantization reduces VRAM by ~4x
**Impact:**
- 12B model: ~24GB ‚Üí ~6GB (model weights)
- Total VRAM: ~14GB (fits in RTX 4080 16GB!)
- Quality: Minimal degradation with 4-bit
**Action:** Use `unsloth/gemma-3-12b-it-unsloth-bnb-4bit` for vLLM testing

---

## üìù Next Steps

### Immediate (Today)
1. ‚úÖ Create `BENCHMARKS.md` on master branch
2. ‚è≥ Create `scripts/ab_test_offline.py` for offline mode testing
3. ‚è≥ Create `scripts/ab_test_server.py` for server mode testing
4. ‚è≥ Create `scripts/compare_ab_results.py` for analysis
5. ‚è≥ Run 6 tests (3 models √ó 2 modes) with `batch_5k.jsonl`

### Short Term (This Week)
6. ‚è≥ Analyze results and choose optimal approach
7. ‚è≥ Document findings in this file
8. ‚è≥ Update both branches with benchmark results
9. ‚è≥ Make decision: Offline vs Server for 170K batch

### Medium Term (This Month)
10. ‚è≥ Run full 170K batch with chosen approach
11. ‚è≥ Validate quality of outputs
12. ‚è≥ Document production recommendations

---

## üìä Raw Benchmark Data

### Test Results (To Be Filled)

#### Test 1: Gemma 12B 4-bit (Offline)
```
Status: ‚è≥ Pending
Model: unsloth/gemma-3-12b-it-unsloth-bnb-4bit
Mode: Offline (LLM.generate)
Dataset: batch_5k.jsonl (5,000 requests)

Results:
- Load Time: -
- Inference Time: -
- Throughput: -
- VRAM Usage: -
- Tokens/sec: -
```

#### Test 2: Mistral 7B (Offline)
```
Status: ‚è≥ Pending
Model: mistralai/Mistral-7B-Instruct-v0.3
Mode: Offline (LLM.generate)
Dataset: batch_5k.jsonl (5,000 requests)

Results:
- Load Time: -
- Inference Time: -
- Throughput: -
- VRAM Usage: -
- Tokens/sec: -
```

#### Test 3: Qwen 14B (Offline)
```
Status: ‚è≥ Pending
Model: Qwen/Qwen2.5-14B-Instruct
Mode: Offline (LLM.generate)
Dataset: batch_5k.jsonl (5,000 requests)

Results:
- Load Time: -
- Inference Time: -
- Throughput: -
- VRAM Usage: -
- Tokens/sec: -
```

#### Test 4: Gemma 12B 4-bit (Server)
```
Status: ‚è≥ Pending
Model: unsloth/gemma-3-12b-it-unsloth-bnb-4bit
Mode: Server (vllm serve + HTTP)
Dataset: batch_5k.jsonl (5,000 requests)

Results:
- Load Time: -
- Inference Time: -
- Throughput: -
- VRAM Usage: -
- Tokens/sec: -
```

#### Test 5: Mistral 7B (Server)
```
Status: ‚è≥ Pending
Model: mistralai/Mistral-7B-Instruct-v0.3
Mode: Server (vllm serve + HTTP)
Dataset: batch_5k.jsonl (5,000 requests)

Results:
- Load Time: -
- Inference Time: -
- Throughput: -
- VRAM Usage: -
- Tokens/sec: -
```

#### Test 6: Qwen 14B (Server)
```
Status: ‚è≥ Pending
Model: Qwen/Qwen2.5-14B-Instruct
Mode: Server (vllm serve + HTTP)
Dataset: batch_5k.jsonl (5,000 requests)

Results:
- Load Time: -
- Inference Time: -
- Throughput: -
- VRAM Usage: -
- Tokens/sec: -
```

---

## üîó Related Documentation

- **Ollama Branch:** See `ollama` branch for consumer GPU implementation
- **vLLM Branch:** See `vllm` branch for production GPU implementation
- **Architecture:** See `AUDIT_AND_VISION.md` on each branch
- **API Docs:** See `docs/API.md` on each branch

---

**Remember:** This file lives on `master` branch and is the **only shared artifact** between branches. Update it after every benchmark run!

